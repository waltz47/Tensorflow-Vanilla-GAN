{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "noise_dim = 100\n",
    "img_sz = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_dataset(data_dir, image_size=(128, 128)):\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    \n",
    "    # Get list of image files\n",
    "    image_files = list(data_dir.glob('*.jpg')) + list(data_dir.glob('*.png'))\n",
    "    image_count = len(image_files)\n",
    "    print(f\"Found {image_count} images.\")\n",
    "\n",
    "    # Convert paths to strings\n",
    "    image_files = [str(path) for path in image_files]\n",
    "\n",
    "    # Create a dataset from the image files\n",
    "    list_ds = tf.data.Dataset.from_tensor_slices(image_files)\n",
    "\n",
    "    # Define preprocessing function\n",
    "    def preprocess_image(file_path):\n",
    "        try:\n",
    "            img = tf.io.read_file(file_path)\n",
    "            img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "            print(f\"Original image shape: {img.shape}\")\n",
    "            img = tf.image.resize(img, image_size, method=tf.image.ResizeMethod.BICUBIC)\n",
    "            print(f\"Resized image shape: {img.shape}\")\n",
    "            img = tf.cast(img, tf.float32)\n",
    "            img = (img - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "            return img\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            print(f\"Error processing image: {file_path}\")\n",
    "            return None\n",
    "\n",
    "    # Map preprocessing function to dataset\n",
    "    dataset = list_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Filter out None values (failed preprocessing)\n",
    "    dataset = dataset.filter(lambda x: x is not None)\n",
    "\n",
    "    # Prepare dataset for training\n",
    "    dataset = dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16*16*256, use_bias=False, input_shape=(noise_dim,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        \n",
    "        tf.keras.layers.Reshape((16, 16, 256)),\n",
    "        \n",
    "        tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(4, 4), padding='same', use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        \n",
    "        # tf.keras.layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        # tf.keras.layers.LeakyReLU(),\n",
    "        \n",
    "        # tf.keras.layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        # tf.keras.layers.LeakyReLU(),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[256,256,3]),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "        # layers.LeakyReLU(),\n",
    "        # layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "        # layers.LeakyReLU(),\n",
    "        # layers.Dropout(0.2),\n",
    "\n",
    "        # layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same'),\n",
    "        # layers.LeakyReLU(),\n",
    "        # layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Conv2D(16, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define loss functions\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Define optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Define the training step\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    \n",
    "    fig = plt.figure(figsize=(32, 32))\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i] * 0.5 + 0.5)  # Denormalize\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.savefig(f'generated/image_at_epoch_{epoch:04d}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Set up the training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        gen_loss_list = []\n",
    "        disc_loss_list = []\n",
    "        for image_batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "            gen_loss_list.append(gen_loss)\n",
    "            disc_loss_list.append(disc_loss)\n",
    "        \n",
    "        # Print losses\n",
    "        print(f\"Epoch {epoch+1}, Gen Loss: {np.mean(gen_loss_list):.4f}, Disc Loss: {np.mean(disc_loss_list):.4f}\")\n",
    "        \n",
    "        # Generate and save images\n",
    "        if (epoch + 1) % 10  == 0:\n",
    "            generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            generator.save(f'generator_model_epoch_{epoch+1}.h5')\n",
    "            discriminator.save(f'discriminator_model_epoch_{epoch+1}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 65536)             6553600   \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 65536)            262144    \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 65536)             0         \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_37 (Conv2D  (None, 32, 32, 128)      819200    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 32, 32, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_76 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_38 (Conv2D  (None, 64, 64, 64)       204800    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 64, 64, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_77 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_39 (Conv2D  (None, 256, 256, 32)     51200     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 256, 256, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 256, 256, 32)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_40 (Conv2D  (None, 256, 256, 3)      2400      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,894,240\n",
      "Trainable params: 7,762,720\n",
      "Non-trainable params: 131,520\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_38 (Conv2D)          (None, 128, 128, 64)      4864      \n",
      "                                                                 \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 64, 64, 256)       409856    \n",
      "                                                                 \n",
      " leaky_re_lu_80 (LeakyReLU)  (None, 64, 64, 256)       0         \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 64, 64, 256)       0         \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 32, 32, 128)       819328    \n",
      "                                                                 \n",
      " leaky_re_lu_81 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 16, 16, 16)        51216     \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,289,361\n",
      "Trainable params: 1,289,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 35 images.\n",
      "Original image shape: (None, None, 3)\n",
      "Resized image shape: (256, 256, 3)\n",
      "Epoch 1, Gen Loss: 0.8135, Disc Loss: 1.1910\n",
      "Epoch 2, Gen Loss: 1.2625, Disc Loss: 0.6231\n",
      "Epoch 3, Gen Loss: 2.0441, Disc Loss: 0.2381\n",
      "Epoch 4, Gen Loss: 3.0774, Disc Loss: 0.1628\n",
      "Epoch 5, Gen Loss: 3.2278, Disc Loss: 0.3291\n",
      "Epoch 6, Gen Loss: 3.4641, Disc Loss: 0.3096\n",
      "Epoch 7, Gen Loss: 2.9212, Disc Loss: 0.3972\n",
      "Epoch 8, Gen Loss: 3.3188, Disc Loss: 0.1986\n",
      "Epoch 9, Gen Loss: 4.4650, Disc Loss: 0.2032\n",
      "Epoch 10, Gen Loss: 3.5292, Disc Loss: 0.0996\n",
      "Epoch 11, Gen Loss: 3.7054, Disc Loss: 0.1851\n",
      "Epoch 12, Gen Loss: 4.9195, Disc Loss: 0.3432\n",
      "Epoch 13, Gen Loss: 6.9377, Disc Loss: 0.6460\n",
      "Epoch 14, Gen Loss: 3.8525, Disc Loss: 0.6911\n",
      "Epoch 15, Gen Loss: 9.7679, Disc Loss: 0.5892\n",
      "Epoch 16, Gen Loss: 2.7471, Disc Loss: 0.2977\n",
      "Epoch 17, Gen Loss: 7.3555, Disc Loss: 0.1257\n",
      "Epoch 18, Gen Loss: 4.8460, Disc Loss: 0.2476\n",
      "Epoch 19, Gen Loss: 3.5430, Disc Loss: 0.1681\n",
      "Epoch 20, Gen Loss: 3.2807, Disc Loss: 0.3410\n",
      "Epoch 21, Gen Loss: 3.6682, Disc Loss: 0.3638\n",
      "Epoch 22, Gen Loss: 3.3678, Disc Loss: 0.3043\n",
      "Epoch 23, Gen Loss: 4.1104, Disc Loss: 0.2653\n",
      "Epoch 24, Gen Loss: 4.4031, Disc Loss: 0.2438\n",
      "Epoch 25, Gen Loss: 4.4882, Disc Loss: 0.1962\n",
      "Epoch 26, Gen Loss: 3.4565, Disc Loss: 0.1451\n",
      "Epoch 27, Gen Loss: 6.6236, Disc Loss: 0.2396\n",
      "Epoch 28, Gen Loss: 2.6292, Disc Loss: 0.2222\n",
      "Epoch 29, Gen Loss: 6.7660, Disc Loss: 0.4079\n",
      "Epoch 30, Gen Loss: 2.5392, Disc Loss: 0.2036\n",
      "Epoch 31, Gen Loss: 4.7672, Disc Loss: 0.0754\n",
      "Epoch 32, Gen Loss: 4.9929, Disc Loss: 0.0821\n",
      "Epoch 33, Gen Loss: 3.1100, Disc Loss: 0.1509\n",
      "Epoch 34, Gen Loss: 5.1241, Disc Loss: 0.0261\n",
      "Epoch 35, Gen Loss: 6.0538, Disc Loss: 0.0205\n",
      "Epoch 36, Gen Loss: 5.5456, Disc Loss: 0.0200\n",
      "Epoch 37, Gen Loss: 4.5183, Disc Loss: 0.0557\n",
      "Epoch 38, Gen Loss: 3.6077, Disc Loss: 0.0443\n",
      "Epoch 39, Gen Loss: 5.2877, Disc Loss: 0.0159\n",
      "Epoch 40, Gen Loss: 5.8705, Disc Loss: 0.0277\n",
      "Epoch 41, Gen Loss: 4.4008, Disc Loss: 0.0785\n",
      "Epoch 42, Gen Loss: 4.0204, Disc Loss: 0.0600\n",
      "Epoch 43, Gen Loss: 6.5856, Disc Loss: 0.0372\n",
      "Epoch 44, Gen Loss: 4.2747, Disc Loss: 0.1098\n",
      "Epoch 45, Gen Loss: 4.6062, Disc Loss: 0.0691\n",
      "Epoch 46, Gen Loss: 5.3630, Disc Loss: 0.0959\n",
      "Epoch 47, Gen Loss: 4.4769, Disc Loss: 0.2634\n",
      "Epoch 48, Gen Loss: 5.6086, Disc Loss: 0.2609\n",
      "Epoch 49, Gen Loss: 3.4533, Disc Loss: 0.1623\n",
      "Epoch 50, Gen Loss: 5.9033, Disc Loss: 0.1118\n",
      "Epoch 51, Gen Loss: 4.9285, Disc Loss: 0.1831\n",
      "Epoch 52, Gen Loss: 5.9801, Disc Loss: 0.1150\n",
      "Epoch 53, Gen Loss: 3.7357, Disc Loss: 0.1205\n",
      "Epoch 54, Gen Loss: 5.2358, Disc Loss: 0.5127\n",
      "Epoch 55, Gen Loss: 5.7439, Disc Loss: 0.4151\n",
      "Epoch 56, Gen Loss: 3.5218, Disc Loss: 0.3717\n",
      "Epoch 57, Gen Loss: 11.8615, Disc Loss: 0.6636\n",
      "Epoch 58, Gen Loss: 7.6196, Disc Loss: 1.0025\n",
      "Epoch 59, Gen Loss: 6.8673, Disc Loss: 0.3682\n",
      "Epoch 60, Gen Loss: 13.2119, Disc Loss: 0.7622\n",
      "Epoch 61, Gen Loss: 4.1948, Disc Loss: 1.6873\n",
      "Epoch 62, Gen Loss: 8.8726, Disc Loss: 1.1231\n",
      "Epoch 63, Gen Loss: 1.6433, Disc Loss: 1.7429\n",
      "Epoch 64, Gen Loss: 5.0548, Disc Loss: 3.4881\n",
      "Epoch 65, Gen Loss: 3.1280, Disc Loss: 1.8049\n",
      "Epoch 66, Gen Loss: 4.1965, Disc Loss: 0.6114\n",
      "Epoch 67, Gen Loss: 6.4363, Disc Loss: 0.2857\n",
      "Epoch 68, Gen Loss: 6.6365, Disc Loss: 0.1480\n",
      "Epoch 69, Gen Loss: 5.2360, Disc Loss: 0.2722\n",
      "Epoch 70, Gen Loss: 3.9267, Disc Loss: 0.2736\n",
      "Epoch 71, Gen Loss: 3.3694, Disc Loss: 0.2806\n",
      "Epoch 72, Gen Loss: 4.2034, Disc Loss: 0.4630\n",
      "Epoch 73, Gen Loss: 2.7300, Disc Loss: 0.5513\n",
      "Epoch 74, Gen Loss: 4.7335, Disc Loss: 0.5186\n",
      "Epoch 75, Gen Loss: 3.2068, Disc Loss: 0.4303\n",
      "Epoch 76, Gen Loss: 4.3377, Disc Loss: 0.3408\n",
      "Epoch 77, Gen Loss: 4.4957, Disc Loss: 0.2147\n",
      "Epoch 78, Gen Loss: 4.4601, Disc Loss: 0.1100\n",
      "Epoch 79, Gen Loss: 3.7811, Disc Loss: 0.1059\n",
      "Epoch 80, Gen Loss: 4.5991, Disc Loss: 0.0896\n",
      "Epoch 81, Gen Loss: 4.0552, Disc Loss: 0.1059\n",
      "Epoch 82, Gen Loss: 3.8159, Disc Loss: 0.0632\n",
      "Epoch 83, Gen Loss: 4.1303, Disc Loss: 0.0721\n",
      "Epoch 84, Gen Loss: 4.5270, Disc Loss: 0.0771\n",
      "Epoch 85, Gen Loss: 3.9156, Disc Loss: 0.2118\n",
      "Epoch 86, Gen Loss: 2.7724, Disc Loss: 0.2027\n",
      "Epoch 87, Gen Loss: 4.2173, Disc Loss: 0.2025\n",
      "Epoch 88, Gen Loss: 3.8339, Disc Loss: 0.1181\n",
      "Epoch 89, Gen Loss: 3.2895, Disc Loss: 0.1787\n",
      "Epoch 90, Gen Loss: 4.8770, Disc Loss: 0.2091\n",
      "Epoch 91, Gen Loss: 4.4374, Disc Loss: 0.1081\n",
      "Epoch 92, Gen Loss: 4.5436, Disc Loss: 0.1725\n",
      "Epoch 93, Gen Loss: 5.2835, Disc Loss: 0.1274\n",
      "Epoch 94, Gen Loss: 4.7631, Disc Loss: 0.1004\n",
      "Epoch 95, Gen Loss: 5.3949, Disc Loss: 0.0709\n",
      "Epoch 96, Gen Loss: 5.4353, Disc Loss: 0.0963\n",
      "Epoch 97, Gen Loss: 4.8508, Disc Loss: 0.0490\n",
      "Epoch 98, Gen Loss: 5.1782, Disc Loss: 0.0465\n",
      "Epoch 99, Gen Loss: 6.1819, Disc Loss: 0.0755\n",
      "Epoch 100, Gen Loss: 4.7824, Disc Loss: 0.1914\n",
      "Epoch 101, Gen Loss: 6.4291, Disc Loss: 0.1668\n",
      "Epoch 102, Gen Loss: 5.2102, Disc Loss: 0.1216\n",
      "Epoch 103, Gen Loss: 5.8770, Disc Loss: 0.1362\n",
      "Epoch 104, Gen Loss: 5.4825, Disc Loss: 0.1245\n",
      "Epoch 105, Gen Loss: 4.5426, Disc Loss: 0.1093\n",
      "Epoch 106, Gen Loss: 4.8957, Disc Loss: 0.3202\n",
      "Epoch 107, Gen Loss: 4.7446, Disc Loss: 0.5654\n",
      "Epoch 108, Gen Loss: 2.3644, Disc Loss: 0.7205\n",
      "Epoch 109, Gen Loss: 4.6666, Disc Loss: 1.0593\n",
      "Epoch 110, Gen Loss: 2.5833, Disc Loss: 0.8023\n",
      "Epoch 111, Gen Loss: 4.7972, Disc Loss: 0.6400\n",
      "Epoch 112, Gen Loss: 2.6775, Disc Loss: 0.3626\n",
      "Epoch 113, Gen Loss: 2.0527, Disc Loss: 0.4319\n",
      "Epoch 114, Gen Loss: 4.5507, Disc Loss: 0.3489\n",
      "Epoch 115, Gen Loss: 3.7058, Disc Loss: 0.2969\n",
      "Epoch 116, Gen Loss: 2.4191, Disc Loss: 0.4715\n",
      "Epoch 117, Gen Loss: 3.1193, Disc Loss: 0.4632\n",
      "Epoch 118, Gen Loss: 3.3654, Disc Loss: 0.5033\n",
      "Epoch 119, Gen Loss: 4.1218, Disc Loss: 0.3639\n",
      "Epoch 120, Gen Loss: 3.2148, Disc Loss: 0.2576\n",
      "Epoch 121, Gen Loss: 4.0205, Disc Loss: 0.1289\n",
      "Epoch 122, Gen Loss: 4.8474, Disc Loss: 0.1767\n",
      "Epoch 123, Gen Loss: 4.7710, Disc Loss: 0.0789\n",
      "Epoch 124, Gen Loss: 3.6812, Disc Loss: 0.1314\n",
      "Epoch 125, Gen Loss: 4.3008, Disc Loss: 0.0883\n",
      "Epoch 126, Gen Loss: 5.1526, Disc Loss: 0.0927\n",
      "Epoch 127, Gen Loss: 3.1850, Disc Loss: 0.3812\n",
      "Epoch 128, Gen Loss: 5.5576, Disc Loss: 1.0280\n",
      "Epoch 129, Gen Loss: 7.6693, Disc Loss: 1.3688\n",
      "Epoch 130, Gen Loss: 3.7867, Disc Loss: 0.2982\n",
      "Epoch 131, Gen Loss: 5.9729, Disc Loss: 0.2465\n",
      "Epoch 132, Gen Loss: 8.1231, Disc Loss: 0.1280\n",
      "Epoch 133, Gen Loss: 5.8744, Disc Loss: 0.3843\n",
      "Epoch 134, Gen Loss: 6.9670, Disc Loss: 0.9273\n",
      "Epoch 135, Gen Loss: 5.5558, Disc Loss: 0.3541\n",
      "Epoch 136, Gen Loss: 7.3595, Disc Loss: 0.1399\n",
      "Epoch 137, Gen Loss: 5.2116, Disc Loss: 0.1784\n",
      "Epoch 138, Gen Loss: 5.7186, Disc Loss: 0.0935\n",
      "Epoch 139, Gen Loss: 6.9841, Disc Loss: 0.3947\n",
      "Epoch 140, Gen Loss: 3.2367, Disc Loss: 0.5436\n",
      "Epoch 141, Gen Loss: 5.5375, Disc Loss: 0.6118\n",
      "Epoch 142, Gen Loss: 3.1368, Disc Loss: 0.7241\n",
      "Epoch 143, Gen Loss: 4.5983, Disc Loss: 0.5930\n",
      "Epoch 144, Gen Loss: 4.5559, Disc Loss: 0.3662\n",
      "Epoch 145, Gen Loss: 3.3066, Disc Loss: 0.4281\n",
      "Epoch 146, Gen Loss: 4.0464, Disc Loss: 0.5674\n",
      "Epoch 147, Gen Loss: 3.0951, Disc Loss: 0.4532\n",
      "Epoch 148, Gen Loss: 3.3472, Disc Loss: 0.5431\n",
      "Epoch 149, Gen Loss: 4.8836, Disc Loss: 0.7663\n",
      "Epoch 150, Gen Loss: 3.0359, Disc Loss: 0.7134\n",
      "Epoch 151, Gen Loss: 3.8284, Disc Loss: 0.7832\n",
      "Epoch 152, Gen Loss: 2.5857, Disc Loss: 0.5113\n",
      "Epoch 153, Gen Loss: 3.2490, Disc Loss: 0.4385\n",
      "Epoch 154, Gen Loss: 3.1283, Disc Loss: 0.3614\n",
      "Epoch 155, Gen Loss: 2.8290, Disc Loss: 0.5270\n",
      "Epoch 156, Gen Loss: 2.1377, Disc Loss: 0.4862\n",
      "Epoch 157, Gen Loss: 3.2911, Disc Loss: 0.3523\n",
      "Epoch 158, Gen Loss: 2.5486, Disc Loss: 0.3952\n",
      "Epoch 159, Gen Loss: 2.5022, Disc Loss: 0.5799\n",
      "Epoch 160, Gen Loss: 1.9516, Disc Loss: 0.7352\n",
      "Epoch 161, Gen Loss: 2.0960, Disc Loss: 0.7363\n",
      "Epoch 162, Gen Loss: 1.5902, Disc Loss: 0.6648\n",
      "Epoch 163, Gen Loss: 3.0090, Disc Loss: 0.6215\n",
      "Epoch 164, Gen Loss: 1.7840, Disc Loss: 0.3811\n",
      "Epoch 165, Gen Loss: 2.8863, Disc Loss: 0.3844\n",
      "Epoch 166, Gen Loss: 2.2074, Disc Loss: 0.3488\n",
      "Epoch 167, Gen Loss: 2.9380, Disc Loss: 0.4429\n",
      "Epoch 168, Gen Loss: 2.4860, Disc Loss: 0.3720\n",
      "Epoch 169, Gen Loss: 2.5563, Disc Loss: 0.4661\n",
      "Epoch 170, Gen Loss: 2.2181, Disc Loss: 0.4831\n",
      "Epoch 171, Gen Loss: 2.3204, Disc Loss: 0.3894\n",
      "Epoch 172, Gen Loss: 2.2538, Disc Loss: 0.4615\n",
      "Epoch 173, Gen Loss: 2.7534, Disc Loss: 0.5625\n",
      "Epoch 174, Gen Loss: 1.6944, Disc Loss: 0.6794\n",
      "Epoch 175, Gen Loss: 2.7174, Disc Loss: 0.7602\n",
      "Epoch 176, Gen Loss: 2.4008, Disc Loss: 0.5305\n",
      "Epoch 177, Gen Loss: 2.3151, Disc Loss: 0.9654\n",
      "Epoch 178, Gen Loss: 2.8231, Disc Loss: 0.9355\n",
      "Epoch 179, Gen Loss: 2.8056, Disc Loss: 0.5002\n",
      "Epoch 180, Gen Loss: 3.2312, Disc Loss: 0.5303\n",
      "Epoch 181, Gen Loss: 3.6443, Disc Loss: 0.6066\n",
      "Epoch 182, Gen Loss: 6.9601, Disc Loss: 0.5789\n",
      "Epoch 183, Gen Loss: 4.1079, Disc Loss: 0.6570\n",
      "Epoch 184, Gen Loss: 2.8398, Disc Loss: 0.9539\n",
      "Epoch 185, Gen Loss: 3.5617, Disc Loss: 1.0878\n",
      "Epoch 186, Gen Loss: 1.8850, Disc Loss: 0.8201\n",
      "Epoch 187, Gen Loss: 1.6096, Disc Loss: 0.5911\n",
      "Epoch 188, Gen Loss: 1.8810, Disc Loss: 0.6985\n",
      "Epoch 189, Gen Loss: 1.8560, Disc Loss: 0.5700\n",
      "Epoch 190, Gen Loss: 1.5631, Disc Loss: 0.5938\n",
      "Epoch 191, Gen Loss: 2.1077, Disc Loss: 0.4043\n",
      "Epoch 192, Gen Loss: 1.9135, Disc Loss: 0.4516\n",
      "Epoch 193, Gen Loss: 2.0580, Disc Loss: 0.3799\n",
      "Epoch 194, Gen Loss: 2.0384, Disc Loss: 0.3380\n",
      "Epoch 195, Gen Loss: 2.2936, Disc Loss: 0.3614\n",
      "Epoch 196, Gen Loss: 1.7694, Disc Loss: 0.4440\n",
      "Epoch 197, Gen Loss: 2.3938, Disc Loss: 0.4552\n",
      "Epoch 198, Gen Loss: 2.1939, Disc Loss: 0.4405\n",
      "Epoch 199, Gen Loss: 1.9733, Disc Loss: 0.4124\n",
      "Epoch 200, Gen Loss: 2.8198, Disc Loss: 0.4424\n",
      "Epoch 201, Gen Loss: 2.1095, Disc Loss: 0.7282\n",
      "Epoch 202, Gen Loss: 1.7265, Disc Loss: 0.6139\n",
      "Epoch 203, Gen Loss: 3.7880, Disc Loss: 1.0967\n",
      "Epoch 204, Gen Loss: 1.5377, Disc Loss: 1.0101\n",
      "Epoch 205, Gen Loss: 2.3549, Disc Loss: 0.6618\n",
      "Epoch 206, Gen Loss: 2.1630, Disc Loss: 0.5161\n",
      "Epoch 207, Gen Loss: 2.4486, Disc Loss: 0.5900\n",
      "Epoch 208, Gen Loss: 2.0230, Disc Loss: 0.4958\n",
      "Epoch 209, Gen Loss: 2.7341, Disc Loss: 0.4354\n",
      "Epoch 210, Gen Loss: 1.7859, Disc Loss: 0.7281\n",
      "Epoch 211, Gen Loss: 3.0684, Disc Loss: 0.8134\n",
      "Epoch 212, Gen Loss: 1.9889, Disc Loss: 0.5226\n",
      "Epoch 213, Gen Loss: 1.7060, Disc Loss: 0.5580\n",
      "Epoch 214, Gen Loss: 2.5331, Disc Loss: 0.5319\n",
      "Epoch 215, Gen Loss: 1.7085, Disc Loss: 0.4198\n",
      "Epoch 216, Gen Loss: 2.0019, Disc Loss: 0.5200\n",
      "Epoch 217, Gen Loss: 1.9194, Disc Loss: 0.4788\n",
      "Epoch 218, Gen Loss: 2.1413, Disc Loss: 0.5196\n",
      "Epoch 219, Gen Loss: 1.7684, Disc Loss: 0.4761\n",
      "Epoch 220, Gen Loss: 2.1668, Disc Loss: 0.4688\n",
      "Epoch 221, Gen Loss: 1.9313, Disc Loss: 0.4908\n",
      "Epoch 222, Gen Loss: 2.0773, Disc Loss: 0.5094\n",
      "Epoch 223, Gen Loss: 2.2364, Disc Loss: 0.4700\n",
      "Epoch 224, Gen Loss: 2.0698, Disc Loss: 0.3882\n",
      "Epoch 225, Gen Loss: 2.6986, Disc Loss: 0.4702\n",
      "Epoch 226, Gen Loss: 2.4149, Disc Loss: 0.5076\n",
      "Epoch 227, Gen Loss: 2.1841, Disc Loss: 0.5747\n",
      "Epoch 228, Gen Loss: 2.7641, Disc Loss: 0.8021\n",
      "Epoch 229, Gen Loss: 1.3987, Disc Loss: 0.8746\n",
      "Epoch 230, Gen Loss: 3.2973, Disc Loss: 0.9889\n",
      "Epoch 231, Gen Loss: 1.3050, Disc Loss: 0.7554\n",
      "Epoch 232, Gen Loss: 1.5787, Disc Loss: 0.6608\n",
      "Epoch 233, Gen Loss: 1.9202, Disc Loss: 0.7383\n",
      "Epoch 234, Gen Loss: 1.6646, Disc Loss: 0.7424\n",
      "Epoch 235, Gen Loss: 1.5907, Disc Loss: 0.7453\n",
      "Epoch 236, Gen Loss: 1.5468, Disc Loss: 0.7809\n",
      "Epoch 237, Gen Loss: 1.5685, Disc Loss: 0.6598\n",
      "Epoch 238, Gen Loss: 1.8129, Disc Loss: 0.6287\n",
      "Epoch 239, Gen Loss: 2.0440, Disc Loss: 0.5378\n",
      "Epoch 240, Gen Loss: 1.5636, Disc Loss: 0.5284\n",
      "Epoch 241, Gen Loss: 1.8897, Disc Loss: 0.5323\n",
      "Epoch 242, Gen Loss: 2.2492, Disc Loss: 0.4484\n",
      "Epoch 243, Gen Loss: 1.8277, Disc Loss: 0.4629\n",
      "Epoch 244, Gen Loss: 2.1093, Disc Loss: 0.4637\n",
      "Epoch 245, Gen Loss: 2.1581, Disc Loss: 0.4336\n",
      "Epoch 246, Gen Loss: 2.0269, Disc Loss: 0.5889\n",
      "Epoch 247, Gen Loss: 1.7307, Disc Loss: 0.8148\n",
      "Epoch 248, Gen Loss: 2.0827, Disc Loss: 0.5946\n",
      "Epoch 249, Gen Loss: 1.3787, Disc Loss: 0.7330\n",
      "Epoch 250, Gen Loss: 1.8643, Disc Loss: 0.7456\n",
      "Epoch 251, Gen Loss: 1.5270, Disc Loss: 0.6486\n",
      "Epoch 252, Gen Loss: 1.7269, Disc Loss: 0.5008\n",
      "Epoch 253, Gen Loss: 1.8997, Disc Loss: 0.5292\n",
      "Epoch 254, Gen Loss: 1.6430, Disc Loss: 0.8967\n",
      "Epoch 255, Gen Loss: 1.3076, Disc Loss: 0.8176\n",
      "Epoch 256, Gen Loss: 1.6450, Disc Loss: 0.6167\n",
      "Epoch 257, Gen Loss: 1.5214, Disc Loss: 0.6828\n",
      "Epoch 258, Gen Loss: 1.5386, Disc Loss: 0.7816\n",
      "Epoch 259, Gen Loss: 1.8221, Disc Loss: 0.7981\n",
      "Epoch 260, Gen Loss: 1.6436, Disc Loss: 0.8525\n",
      "Epoch 261, Gen Loss: 1.6923, Disc Loss: 0.9062\n",
      "Epoch 262, Gen Loss: 1.8823, Disc Loss: 0.7273\n",
      "Epoch 263, Gen Loss: 2.0985, Disc Loss: 0.7384\n",
      "Epoch 264, Gen Loss: 1.8267, Disc Loss: 0.5761\n",
      "Epoch 265, Gen Loss: 2.1987, Disc Loss: 0.4903\n",
      "Epoch 266, Gen Loss: 2.2876, Disc Loss: 0.6505\n",
      "Epoch 267, Gen Loss: 1.8456, Disc Loss: 0.8422\n",
      "Epoch 268, Gen Loss: 1.1270, Disc Loss: 1.0363\n",
      "Epoch 269, Gen Loss: 1.7347, Disc Loss: 0.7607\n",
      "Epoch 270, Gen Loss: 1.6611, Disc Loss: 0.7338\n",
      "Epoch 271, Gen Loss: 1.7194, Disc Loss: 0.7598\n",
      "Epoch 272, Gen Loss: 1.1283, Disc Loss: 0.8702\n",
      "Epoch 273, Gen Loss: 1.8828, Disc Loss: 0.5671\n",
      "Epoch 274, Gen Loss: 1.9158, Disc Loss: 0.5420\n",
      "Epoch 275, Gen Loss: 1.4627, Disc Loss: 0.5404\n",
      "Epoch 276, Gen Loss: 1.8796, Disc Loss: 1.0448\n",
      "Epoch 277, Gen Loss: 1.0166, Disc Loss: 1.4220\n",
      "Epoch 278, Gen Loss: 1.5536, Disc Loss: 1.1221\n",
      "Epoch 279, Gen Loss: 1.5816, Disc Loss: 0.7539\n",
      "Epoch 280, Gen Loss: 1.5173, Disc Loss: 0.6978\n",
      "Epoch 281, Gen Loss: 1.9322, Disc Loss: 0.4753\n",
      "Epoch 282, Gen Loss: 1.5197, Disc Loss: 0.6339\n",
      "Epoch 283, Gen Loss: 1.8087, Disc Loss: 0.5702\n",
      "Epoch 284, Gen Loss: 1.4100, Disc Loss: 0.7847\n",
      "Epoch 285, Gen Loss: 1.4725, Disc Loss: 0.8291\n",
      "Epoch 286, Gen Loss: 1.4884, Disc Loss: 0.5609\n",
      "Epoch 287, Gen Loss: 2.1688, Disc Loss: 0.6133\n",
      "Epoch 288, Gen Loss: 1.4470, Disc Loss: 0.5620\n",
      "Epoch 289, Gen Loss: 1.6803, Disc Loss: 0.6037\n",
      "Epoch 290, Gen Loss: 1.5231, Disc Loss: 0.9220\n",
      "Epoch 291, Gen Loss: 1.1190, Disc Loss: 0.9573\n",
      "Epoch 292, Gen Loss: 1.7463, Disc Loss: 0.6394\n",
      "Epoch 293, Gen Loss: 1.7890, Disc Loss: 0.5465\n",
      "Epoch 294, Gen Loss: 2.4288, Disc Loss: 0.3708\n",
      "Epoch 295, Gen Loss: 1.8677, Disc Loss: 0.5107\n",
      "Epoch 296, Gen Loss: 2.0508, Disc Loss: 0.8229\n",
      "Epoch 297, Gen Loss: 1.2351, Disc Loss: 0.8716\n",
      "Epoch 298, Gen Loss: 1.5603, Disc Loss: 1.1825\n",
      "Epoch 299, Gen Loss: 1.0964, Disc Loss: 1.0370\n",
      "Epoch 300, Gen Loss: 1.4275, Disc Loss: 0.7784\n",
      "Epoch 301, Gen Loss: 1.5391, Disc Loss: 0.5830\n",
      "Epoch 302, Gen Loss: 1.9183, Disc Loss: 0.4347\n",
      "Epoch 303, Gen Loss: 2.3359, Disc Loss: 0.3460\n",
      "Epoch 304, Gen Loss: 2.0300, Disc Loss: 0.3682\n",
      "Epoch 305, Gen Loss: 2.3301, Disc Loss: 0.5926\n",
      "Epoch 306, Gen Loss: 1.8827, Disc Loss: 0.7645\n",
      "Epoch 307, Gen Loss: 1.6687, Disc Loss: 0.5988\n",
      "Epoch 308, Gen Loss: 1.6294, Disc Loss: 0.6467\n",
      "Epoch 309, Gen Loss: 2.0790, Disc Loss: 0.3774\n",
      "Epoch 310, Gen Loss: 2.5856, Disc Loss: 0.3410\n",
      "Epoch 311, Gen Loss: 2.0292, Disc Loss: 0.7074\n",
      "Epoch 312, Gen Loss: 1.7249, Disc Loss: 1.1756\n",
      "Epoch 313, Gen Loss: 1.6490, Disc Loss: 0.9785\n",
      "Epoch 314, Gen Loss: 1.9392, Disc Loss: 0.5063\n",
      "Epoch 315, Gen Loss: 3.1042, Disc Loss: 0.2806\n",
      "Epoch 316, Gen Loss: 1.8486, Disc Loss: 0.4249\n",
      "Epoch 317, Gen Loss: 2.6309, Disc Loss: 0.7420\n",
      "Epoch 318, Gen Loss: 1.7180, Disc Loss: 1.6199\n",
      "Epoch 319, Gen Loss: 1.3212, Disc Loss: 2.3645\n",
      "Epoch 320, Gen Loss: 0.9731, Disc Loss: 1.6149\n",
      "Epoch 321, Gen Loss: 1.6581, Disc Loss: 1.4693\n",
      "Epoch 322, Gen Loss: 1.0970, Disc Loss: 1.0125\n",
      "Epoch 323, Gen Loss: 1.2931, Disc Loss: 0.7539\n",
      "Epoch 324, Gen Loss: 1.7351, Disc Loss: 0.9322\n",
      "Epoch 325, Gen Loss: 1.2104, Disc Loss: 0.7370\n",
      "Epoch 326, Gen Loss: 1.5456, Disc Loss: 0.7620\n",
      "Epoch 327, Gen Loss: 1.8190, Disc Loss: 0.5402\n",
      "Epoch 328, Gen Loss: 2.4433, Disc Loss: 0.7587\n",
      "Epoch 329, Gen Loss: 2.0361, Disc Loss: 1.6479\n",
      "Epoch 330, Gen Loss: 1.3330, Disc Loss: 1.7584\n",
      "Epoch 331, Gen Loss: 1.8604, Disc Loss: 1.1215\n",
      "Epoch 332, Gen Loss: 1.7230, Disc Loss: 0.8774\n",
      "Epoch 333, Gen Loss: 1.8284, Disc Loss: 0.8179\n",
      "Epoch 334, Gen Loss: 1.7753, Disc Loss: 0.7751\n",
      "Epoch 335, Gen Loss: 2.3111, Disc Loss: 0.7156\n",
      "Epoch 336, Gen Loss: 2.4833, Disc Loss: 0.6098\n",
      "Epoch 337, Gen Loss: 2.4340, Disc Loss: 0.4445\n",
      "Epoch 338, Gen Loss: 3.3333, Disc Loss: 0.4419\n",
      "Epoch 339, Gen Loss: 2.7220, Disc Loss: 0.4075\n",
      "Epoch 340, Gen Loss: 2.7553, Disc Loss: 0.4748\n",
      "Epoch 341, Gen Loss: 2.4118, Disc Loss: 0.6402\n",
      "Epoch 342, Gen Loss: 1.6318, Disc Loss: 1.2656\n",
      "Epoch 343, Gen Loss: 1.3794, Disc Loss: 0.9455\n",
      "Epoch 344, Gen Loss: 1.6534, Disc Loss: 1.0475\n",
      "Epoch 345, Gen Loss: 0.7817, Disc Loss: 1.0847\n",
      "Epoch 346, Gen Loss: 1.1795, Disc Loss: 1.1180\n",
      "Epoch 347, Gen Loss: 0.8133, Disc Loss: 1.0356\n",
      "Epoch 348, Gen Loss: 1.1862, Disc Loss: 0.8719\n",
      "Epoch 349, Gen Loss: 1.2029, Disc Loss: 0.8347\n",
      "Epoch 350, Gen Loss: 1.1134, Disc Loss: 0.8242\n",
      "Epoch 351, Gen Loss: 1.2625, Disc Loss: 0.7655\n",
      "Epoch 352, Gen Loss: 1.2746, Disc Loss: 0.8393\n",
      "Epoch 353, Gen Loss: 1.2536, Disc Loss: 0.7800\n",
      "Epoch 354, Gen Loss: 1.2913, Disc Loss: 0.7506\n",
      "Epoch 355, Gen Loss: 1.3834, Disc Loss: 0.7249\n",
      "Epoch 356, Gen Loss: 1.2494, Disc Loss: 0.6604\n",
      "Epoch 357, Gen Loss: 1.3538, Disc Loss: 0.7557\n",
      "Epoch 358, Gen Loss: 1.3170, Disc Loss: 0.6939\n",
      "Epoch 359, Gen Loss: 1.5157, Disc Loss: 0.7569\n",
      "Epoch 360, Gen Loss: 1.0574, Disc Loss: 0.7390\n",
      "Epoch 361, Gen Loss: 1.7822, Disc Loss: 0.8226\n",
      "Epoch 362, Gen Loss: 0.9904, Disc Loss: 0.8742\n",
      "Epoch 363, Gen Loss: 1.4044, Disc Loss: 0.7724\n",
      "Epoch 364, Gen Loss: 1.3449, Disc Loss: 0.9782\n",
      "Epoch 365, Gen Loss: 0.9355, Disc Loss: 1.0110\n",
      "Epoch 366, Gen Loss: 1.3656, Disc Loss: 0.8837\n",
      "Epoch 367, Gen Loss: 1.3846, Disc Loss: 0.7398\n",
      "Epoch 368, Gen Loss: 1.9168, Disc Loss: 0.5388\n",
      "Epoch 369, Gen Loss: 2.2494, Disc Loss: 0.3746\n",
      "Epoch 370, Gen Loss: 2.2632, Disc Loss: 0.5546\n",
      "Epoch 371, Gen Loss: 1.7271, Disc Loss: 0.6773\n",
      "Epoch 372, Gen Loss: 1.2247, Disc Loss: 1.2682\n",
      "Epoch 373, Gen Loss: 0.8740, Disc Loss: 1.2904\n",
      "Epoch 374, Gen Loss: 1.6616, Disc Loss: 0.7979\n",
      "Epoch 375, Gen Loss: 1.4242, Disc Loss: 0.5664\n",
      "Epoch 376, Gen Loss: 1.7243, Disc Loss: 0.5181\n",
      "Epoch 377, Gen Loss: 1.5150, Disc Loss: 0.7230\n",
      "Epoch 378, Gen Loss: 0.8640, Disc Loss: 1.0344\n",
      "Epoch 379, Gen Loss: 1.1710, Disc Loss: 1.4453\n",
      "Epoch 380, Gen Loss: 0.5029, Disc Loss: 1.5014\n",
      "Epoch 381, Gen Loss: 2.0173, Disc Loss: 0.6682\n",
      "Epoch 382, Gen Loss: 2.1072, Disc Loss: 0.5644\n",
      "Epoch 383, Gen Loss: 2.4598, Disc Loss: 0.8887\n",
      "Epoch 384, Gen Loss: 1.6011, Disc Loss: 0.5951\n",
      "Epoch 385, Gen Loss: 2.1418, Disc Loss: 0.6050\n",
      "Epoch 386, Gen Loss: 1.5318, Disc Loss: 0.8027\n",
      "Epoch 387, Gen Loss: 1.2666, Disc Loss: 0.9995\n",
      "Epoch 388, Gen Loss: 1.3234, Disc Loss: 1.3036\n",
      "Epoch 389, Gen Loss: 0.8858, Disc Loss: 0.9554\n",
      "Epoch 390, Gen Loss: 1.2525, Disc Loss: 0.7018\n",
      "Epoch 391, Gen Loss: 1.9474, Disc Loss: 0.5181\n",
      "Epoch 392, Gen Loss: 1.8992, Disc Loss: 0.4178\n",
      "Epoch 393, Gen Loss: 1.5359, Disc Loss: 0.4105\n",
      "Epoch 394, Gen Loss: 1.8124, Disc Loss: 0.5097\n",
      "Epoch 395, Gen Loss: 1.4572, Disc Loss: 0.7580\n",
      "Epoch 396, Gen Loss: 1.6222, Disc Loss: 0.8369\n",
      "Epoch 397, Gen Loss: 1.6917, Disc Loss: 0.8592\n",
      "Epoch 398, Gen Loss: 1.7742, Disc Loss: 0.7309\n",
      "Epoch 399, Gen Loss: 2.3658, Disc Loss: 0.5415\n",
      "Epoch 400, Gen Loss: 2.1385, Disc Loss: 0.4931\n",
      "Epoch 401, Gen Loss: 1.9647, Disc Loss: 0.4575\n",
      "Epoch 402, Gen Loss: 2.0062, Disc Loss: 0.5449\n",
      "Epoch 403, Gen Loss: 1.7910, Disc Loss: 0.8677\n",
      "Epoch 404, Gen Loss: 1.2607, Disc Loss: 1.1463\n",
      "Epoch 405, Gen Loss: 1.2400, Disc Loss: 1.1040\n",
      "Epoch 406, Gen Loss: 1.3708, Disc Loss: 0.9333\n",
      "Epoch 407, Gen Loss: 1.2106, Disc Loss: 0.6913\n",
      "Epoch 408, Gen Loss: 1.2959, Disc Loss: 0.5574\n",
      "Epoch 409, Gen Loss: 2.0568, Disc Loss: 0.5971\n",
      "Epoch 410, Gen Loss: 2.1230, Disc Loss: 0.4172\n",
      "Epoch 411, Gen Loss: 1.3979, Disc Loss: 0.5807\n",
      "Epoch 412, Gen Loss: 1.4755, Disc Loss: 0.6503\n",
      "Epoch 413, Gen Loss: 0.9905, Disc Loss: 1.0662\n",
      "Epoch 414, Gen Loss: 1.7127, Disc Loss: 0.7408\n",
      "Epoch 415, Gen Loss: 1.9364, Disc Loss: 0.3805\n",
      "Epoch 416, Gen Loss: 2.1019, Disc Loss: 0.3783\n",
      "Epoch 417, Gen Loss: 2.0086, Disc Loss: 0.7892\n",
      "Epoch 418, Gen Loss: 1.1117, Disc Loss: 1.3827\n",
      "Epoch 419, Gen Loss: 1.1238, Disc Loss: 1.1197\n",
      "Epoch 420, Gen Loss: 1.4891, Disc Loss: 0.6276\n",
      "Epoch 421, Gen Loss: 2.0968, Disc Loss: 0.3857\n",
      "Epoch 422, Gen Loss: 1.6921, Disc Loss: 0.5827\n",
      "Epoch 423, Gen Loss: 1.4317, Disc Loss: 1.2097\n",
      "Epoch 424, Gen Loss: 0.8880, Disc Loss: 1.4536\n",
      "Epoch 425, Gen Loss: 1.3360, Disc Loss: 0.9303\n",
      "Epoch 426, Gen Loss: 2.3954, Disc Loss: 0.4239\n",
      "Epoch 427, Gen Loss: 2.5463, Disc Loss: 0.3772\n",
      "Epoch 428, Gen Loss: 1.5035, Disc Loss: 0.5002\n",
      "Epoch 429, Gen Loss: 2.0499, Disc Loss: 0.7276\n",
      "Epoch 430, Gen Loss: 1.3755, Disc Loss: 0.6756\n",
      "Epoch 431, Gen Loss: 1.6324, Disc Loss: 0.7785\n",
      "Epoch 432, Gen Loss: 1.4734, Disc Loss: 0.8528\n",
      "Epoch 433, Gen Loss: 1.2732, Disc Loss: 0.9293\n",
      "Epoch 434, Gen Loss: 1.1624, Disc Loss: 0.8037\n",
      "Epoch 435, Gen Loss: 1.2232, Disc Loss: 0.7939\n",
      "Epoch 436, Gen Loss: 1.8524, Disc Loss: 0.6254\n",
      "Epoch 437, Gen Loss: 1.9755, Disc Loss: 0.4541\n",
      "Epoch 438, Gen Loss: 1.8180, Disc Loss: 0.5188\n",
      "Epoch 439, Gen Loss: 1.7204, Disc Loss: 0.6962\n",
      "Epoch 440, Gen Loss: 1.4154, Disc Loss: 0.9672\n",
      "Epoch 441, Gen Loss: 2.3776, Disc Loss: 0.5835\n",
      "Epoch 442, Gen Loss: 2.1850, Disc Loss: 0.3275\n",
      "Epoch 443, Gen Loss: 2.6256, Disc Loss: 0.2631\n",
      "Epoch 444, Gen Loss: 2.5620, Disc Loss: 0.3215\n",
      "Epoch 445, Gen Loss: 1.7358, Disc Loss: 0.4297\n",
      "Epoch 446, Gen Loss: 1.9859, Disc Loss: 0.6876\n",
      "Epoch 447, Gen Loss: 1.4764, Disc Loss: 0.8327\n",
      "Epoch 448, Gen Loss: 1.2888, Disc Loss: 0.9512\n",
      "Epoch 449, Gen Loss: 1.0861, Disc Loss: 1.1601\n",
      "Epoch 450, Gen Loss: 1.4238, Disc Loss: 0.7502\n",
      "Epoch 451, Gen Loss: 2.2316, Disc Loss: 0.3530\n",
      "Epoch 452, Gen Loss: 1.9825, Disc Loss: 0.3190\n",
      "Epoch 453, Gen Loss: 1.7669, Disc Loss: 0.5444\n",
      "Epoch 454, Gen Loss: 1.1124, Disc Loss: 0.9271\n",
      "Epoch 455, Gen Loss: 1.0079, Disc Loss: 1.0424\n",
      "Epoch 456, Gen Loss: 1.2538, Disc Loss: 0.8471\n",
      "Epoch 457, Gen Loss: 1.6321, Disc Loss: 0.4995\n",
      "Epoch 458, Gen Loss: 1.8174, Disc Loss: 0.5606\n",
      "Epoch 459, Gen Loss: 1.8316, Disc Loss: 0.6394\n",
      "Epoch 460, Gen Loss: 0.9584, Disc Loss: 0.8709\n",
      "Epoch 461, Gen Loss: 1.1593, Disc Loss: 0.8894\n",
      "Epoch 462, Gen Loss: 1.3567, Disc Loss: 0.7413\n",
      "Epoch 463, Gen Loss: 1.4376, Disc Loss: 0.7335\n",
      "Epoch 464, Gen Loss: 1.3285, Disc Loss: 0.6065\n",
      "Epoch 465, Gen Loss: 1.7181, Disc Loss: 0.4568\n",
      "Epoch 466, Gen Loss: 1.8630, Disc Loss: 0.4821\n",
      "Epoch 467, Gen Loss: 1.7167, Disc Loss: 0.5685\n",
      "Epoch 468, Gen Loss: 1.5345, Disc Loss: 0.6118\n",
      "Epoch 469, Gen Loss: 1.5991, Disc Loss: 0.6392\n",
      "Epoch 470, Gen Loss: 2.2707, Disc Loss: 0.4772\n",
      "Epoch 471, Gen Loss: 2.2534, Disc Loss: 0.5051\n",
      "Epoch 472, Gen Loss: 1.9471, Disc Loss: 0.5434\n",
      "Epoch 473, Gen Loss: 1.4653, Disc Loss: 0.9904\n",
      "Epoch 474, Gen Loss: 1.8644, Disc Loss: 1.1568\n",
      "Epoch 475, Gen Loss: 1.8020, Disc Loss: 0.9446\n",
      "Epoch 476, Gen Loss: 1.3257, Disc Loss: 0.5163\n",
      "Epoch 477, Gen Loss: 2.6201, Disc Loss: 0.5226\n",
      "Epoch 478, Gen Loss: 1.4431, Disc Loss: 0.5303\n",
      "Epoch 479, Gen Loss: 2.1673, Disc Loss: 0.4838\n",
      "Epoch 480, Gen Loss: 1.2795, Disc Loss: 0.7034\n",
      "Epoch 481, Gen Loss: 1.7620, Disc Loss: 0.5982\n",
      "Epoch 482, Gen Loss: 1.8016, Disc Loss: 0.5630\n",
      "Epoch 483, Gen Loss: 1.9786, Disc Loss: 0.3696\n",
      "Epoch 484, Gen Loss: 2.4698, Disc Loss: 0.3756\n",
      "Epoch 485, Gen Loss: 1.7914, Disc Loss: 0.5259\n",
      "Epoch 486, Gen Loss: 1.5557, Disc Loss: 0.9680\n",
      "Epoch 487, Gen Loss: 1.7785, Disc Loss: 0.6718\n",
      "Epoch 488, Gen Loss: 2.0182, Disc Loss: 0.2781\n",
      "Epoch 489, Gen Loss: 2.7593, Disc Loss: 0.1987\n",
      "Epoch 490, Gen Loss: 2.2824, Disc Loss: 0.2161\n",
      "Epoch 491, Gen Loss: 2.4869, Disc Loss: 0.3995\n",
      "Epoch 492, Gen Loss: 1.5110, Disc Loss: 0.7572\n",
      "Epoch 493, Gen Loss: 1.4419, Disc Loss: 0.9835\n",
      "Epoch 494, Gen Loss: 1.6477, Disc Loss: 0.8257\n",
      "Epoch 495, Gen Loss: 2.4944, Disc Loss: 0.3715\n",
      "Epoch 496, Gen Loss: 2.3984, Disc Loss: 0.4933\n",
      "Epoch 497, Gen Loss: 1.6265, Disc Loss: 0.6336\n",
      "Epoch 498, Gen Loss: 2.1900, Disc Loss: 0.6367\n",
      "Epoch 499, Gen Loss: 1.9989, Disc Loss: 0.6633\n",
      "Epoch 500, Gen Loss: 1.6545, Disc Loss: 0.5838\n",
      "Epoch 501, Gen Loss: 2.4527, Disc Loss: 0.4610\n",
      "Epoch 502, Gen Loss: 2.0934, Disc Loss: 0.5008\n",
      "Epoch 503, Gen Loss: 1.6445, Disc Loss: 0.4852\n",
      "Epoch 504, Gen Loss: 1.4873, Disc Loss: 0.9366\n",
      "Epoch 505, Gen Loss: 1.8207, Disc Loss: 0.7420\n",
      "Epoch 506, Gen Loss: 2.2022, Disc Loss: 0.6340\n",
      "Epoch 507, Gen Loss: 2.0445, Disc Loss: 0.4273\n",
      "Epoch 508, Gen Loss: 3.2907, Disc Loss: 0.3989\n",
      "Epoch 509, Gen Loss: 1.3839, Disc Loss: 0.5107\n",
      "Epoch 510, Gen Loss: 2.6742, Disc Loss: 0.4676\n",
      "Epoch 511, Gen Loss: 1.6428, Disc Loss: 0.4998\n",
      "Epoch 512, Gen Loss: 1.9159, Disc Loss: 0.5350\n",
      "Epoch 513, Gen Loss: 1.7526, Disc Loss: 0.4191\n",
      "Epoch 514, Gen Loss: 1.9923, Disc Loss: 0.5590\n",
      "Epoch 515, Gen Loss: 1.8600, Disc Loss: 0.3692\n",
      "Epoch 516, Gen Loss: 1.9768, Disc Loss: 0.4894\n",
      "Epoch 517, Gen Loss: 1.9253, Disc Loss: 0.5095\n",
      "Epoch 518, Gen Loss: 1.6226, Disc Loss: 0.5913\n",
      "Epoch 519, Gen Loss: 1.6326, Disc Loss: 0.5919\n",
      "Epoch 520, Gen Loss: 1.7661, Disc Loss: 0.6457\n",
      "Epoch 521, Gen Loss: 2.0215, Disc Loss: 0.4956\n",
      "Epoch 522, Gen Loss: 1.8947, Disc Loss: 0.4423\n",
      "Epoch 523, Gen Loss: 1.7728, Disc Loss: 0.3992\n",
      "Epoch 524, Gen Loss: 1.8902, Disc Loss: 0.5924\n",
      "Epoch 525, Gen Loss: 1.8874, Disc Loss: 0.7070\n",
      "Epoch 526, Gen Loss: 1.9760, Disc Loss: 0.4501\n",
      "Epoch 527, Gen Loss: 2.2385, Disc Loss: 0.4977\n",
      "Epoch 528, Gen Loss: 1.9096, Disc Loss: 0.4555\n",
      "Epoch 529, Gen Loss: 2.7180, Disc Loss: 0.3908\n",
      "Epoch 530, Gen Loss: 1.5164, Disc Loss: 0.6575\n",
      "Epoch 531, Gen Loss: 1.4869, Disc Loss: 0.7046\n",
      "Epoch 532, Gen Loss: 1.8533, Disc Loss: 0.6195\n",
      "Epoch 533, Gen Loss: 2.1282, Disc Loss: 0.5108\n",
      "Epoch 534, Gen Loss: 2.2786, Disc Loss: 0.4863\n",
      "Epoch 535, Gen Loss: 2.7092, Disc Loss: 0.4918\n",
      "Epoch 536, Gen Loss: 2.6114, Disc Loss: 1.0450\n",
      "Epoch 537, Gen Loss: 1.6009, Disc Loss: 1.0241\n",
      "Epoch 538, Gen Loss: 2.1829, Disc Loss: 0.6991\n",
      "Epoch 539, Gen Loss: 1.6641, Disc Loss: 0.5125\n",
      "Epoch 540, Gen Loss: 2.4224, Disc Loss: 0.3838\n",
      "Epoch 541, Gen Loss: 1.7875, Disc Loss: 0.5193\n",
      "Epoch 542, Gen Loss: 2.9370, Disc Loss: 0.4284\n",
      "Epoch 543, Gen Loss: 1.6118, Disc Loss: 0.5433\n",
      "Epoch 544, Gen Loss: 2.3620, Disc Loss: 0.5550\n",
      "Epoch 545, Gen Loss: 2.0020, Disc Loss: 0.4214\n",
      "Epoch 546, Gen Loss: 2.0817, Disc Loss: 0.4496\n",
      "Epoch 547, Gen Loss: 1.9023, Disc Loss: 0.8205\n",
      "Epoch 548, Gen Loss: 1.3764, Disc Loss: 0.7390\n",
      "Epoch 549, Gen Loss: 1.7171, Disc Loss: 0.7128\n",
      "Epoch 550, Gen Loss: 1.6531, Disc Loss: 0.6681\n",
      "Epoch 551, Gen Loss: 2.0508, Disc Loss: 0.4097\n",
      "Epoch 552, Gen Loss: 1.8327, Disc Loss: 0.4469\n",
      "Epoch 553, Gen Loss: 1.9026, Disc Loss: 0.5299\n",
      "Epoch 554, Gen Loss: 1.6886, Disc Loss: 0.6229\n",
      "Epoch 555, Gen Loss: 1.5257, Disc Loss: 0.6495\n",
      "Epoch 556, Gen Loss: 1.7741, Disc Loss: 0.5386\n",
      "Epoch 557, Gen Loss: 2.0195, Disc Loss: 0.4139\n",
      "Epoch 558, Gen Loss: 2.5988, Disc Loss: 0.3579\n",
      "Epoch 559, Gen Loss: 2.3241, Disc Loss: 0.3427\n",
      "Epoch 560, Gen Loss: 2.2126, Disc Loss: 0.7533\n",
      "Epoch 561, Gen Loss: 1.4969, Disc Loss: 0.9064\n",
      "Epoch 562, Gen Loss: 1.4152, Disc Loss: 0.6663\n",
      "Epoch 563, Gen Loss: 2.8111, Disc Loss: 0.3253\n",
      "Epoch 564, Gen Loss: 1.9935, Disc Loss: 0.2939\n",
      "Epoch 565, Gen Loss: 2.2459, Disc Loss: 0.4761\n",
      "Epoch 566, Gen Loss: 1.4550, Disc Loss: 0.7735\n",
      "Epoch 567, Gen Loss: 1.6534, Disc Loss: 0.7844\n",
      "Epoch 568, Gen Loss: 2.3109, Disc Loss: 0.6029\n",
      "Epoch 569, Gen Loss: 1.7884, Disc Loss: 0.6891\n",
      "Epoch 570, Gen Loss: 1.8889, Disc Loss: 0.3711\n",
      "Epoch 571, Gen Loss: 2.7365, Disc Loss: 0.7022\n",
      "Epoch 572, Gen Loss: 1.1226, Disc Loss: 0.8622\n",
      "Epoch 573, Gen Loss: 1.7253, Disc Loss: 0.7339\n",
      "Epoch 574, Gen Loss: 1.4115, Disc Loss: 0.6600\n",
      "Epoch 575, Gen Loss: 1.3720, Disc Loss: 0.6062\n",
      "Epoch 576, Gen Loss: 1.7267, Disc Loss: 0.6466\n",
      "Epoch 577, Gen Loss: 1.8176, Disc Loss: 0.5195\n",
      "Epoch 578, Gen Loss: 1.7699, Disc Loss: 0.4485\n",
      "Epoch 579, Gen Loss: 1.8767, Disc Loss: 0.5778\n",
      "Epoch 580, Gen Loss: 1.6174, Disc Loss: 0.6739\n",
      "Epoch 581, Gen Loss: 1.9376, Disc Loss: 0.6141\n",
      "Epoch 582, Gen Loss: 2.1884, Disc Loss: 0.6101\n",
      "Epoch 583, Gen Loss: 1.5875, Disc Loss: 0.5215\n",
      "Epoch 584, Gen Loss: 1.6731, Disc Loss: 0.6454\n",
      "Epoch 585, Gen Loss: 1.7596, Disc Loss: 0.6415\n",
      "Epoch 586, Gen Loss: 2.0648, Disc Loss: 0.5215\n",
      "Epoch 587, Gen Loss: 1.8329, Disc Loss: 0.8351\n",
      "Epoch 588, Gen Loss: 1.3085, Disc Loss: 0.7567\n",
      "Epoch 589, Gen Loss: 2.1165, Disc Loss: 0.7823\n",
      "Epoch 590, Gen Loss: 1.2161, Disc Loss: 0.6682\n",
      "Epoch 591, Gen Loss: 2.0526, Disc Loss: 0.5776\n",
      "Epoch 592, Gen Loss: 1.5165, Disc Loss: 0.5428\n",
      "Epoch 593, Gen Loss: 1.9845, Disc Loss: 0.4831\n",
      "Epoch 594, Gen Loss: 1.9905, Disc Loss: 0.4449\n",
      "Epoch 595, Gen Loss: 1.6655, Disc Loss: 0.5789\n",
      "Epoch 596, Gen Loss: 1.9393, Disc Loss: 0.5042\n",
      "Epoch 597, Gen Loss: 1.8791, Disc Loss: 0.5024\n",
      "Epoch 598, Gen Loss: 1.7377, Disc Loss: 0.5896\n",
      "Epoch 599, Gen Loss: 1.9364, Disc Loss: 0.5628\n",
      "Epoch 600, Gen Loss: 1.7135, Disc Loss: 0.7087\n",
      "Epoch 601, Gen Loss: 1.8520, Disc Loss: 0.7149\n",
      "Epoch 602, Gen Loss: 1.7569, Disc Loss: 0.7405\n",
      "Epoch 603, Gen Loss: 1.3716, Disc Loss: 0.5390\n",
      "Epoch 604, Gen Loss: 2.4174, Disc Loss: 0.6257\n",
      "Epoch 605, Gen Loss: 1.1467, Disc Loss: 0.5885\n",
      "Epoch 606, Gen Loss: 1.8525, Disc Loss: 0.6606\n",
      "Epoch 607, Gen Loss: 1.5482, Disc Loss: 0.5586\n",
      "Epoch 608, Gen Loss: 1.9896, Disc Loss: 0.3779\n",
      "Epoch 609, Gen Loss: 2.1160, Disc Loss: 0.4473\n",
      "Epoch 610, Gen Loss: 2.4773, Disc Loss: 0.5784\n",
      "Epoch 611, Gen Loss: 1.8777, Disc Loss: 0.7129\n",
      "Epoch 612, Gen Loss: 2.2521, Disc Loss: 0.6237\n",
      "Epoch 613, Gen Loss: 1.3919, Disc Loss: 0.6517\n",
      "Epoch 614, Gen Loss: 1.9203, Disc Loss: 0.6743\n",
      "Epoch 615, Gen Loss: 1.2307, Disc Loss: 0.6876\n",
      "Epoch 616, Gen Loss: 1.5998, Disc Loss: 0.7526\n",
      "Epoch 617, Gen Loss: 1.9346, Disc Loss: 0.6552\n",
      "Epoch 618, Gen Loss: 2.2856, Disc Loss: 0.5222\n",
      "Epoch 619, Gen Loss: 1.9116, Disc Loss: 0.4673\n",
      "Epoch 620, Gen Loss: 2.1561, Disc Loss: 0.4679\n",
      "Epoch 621, Gen Loss: 1.7218, Disc Loss: 0.8193\n",
      "Epoch 622, Gen Loss: 1.2463, Disc Loss: 0.9197\n",
      "Epoch 623, Gen Loss: 2.0004, Disc Loss: 0.7718\n",
      "Epoch 624, Gen Loss: 1.9846, Disc Loss: 0.4667\n",
      "Epoch 625, Gen Loss: 1.9667, Disc Loss: 0.6056\n",
      "Epoch 626, Gen Loss: 1.4772, Disc Loss: 0.5274\n",
      "Epoch 627, Gen Loss: 1.6721, Disc Loss: 0.5685\n",
      "Epoch 628, Gen Loss: 1.6738, Disc Loss: 0.5400\n",
      "Epoch 629, Gen Loss: 1.9742, Disc Loss: 0.4006\n",
      "Epoch 630, Gen Loss: 1.9703, Disc Loss: 0.4881\n",
      "Epoch 631, Gen Loss: 2.1334, Disc Loss: 0.4774\n",
      "Epoch 632, Gen Loss: 2.1277, Disc Loss: 0.4965\n",
      "Epoch 633, Gen Loss: 1.7473, Disc Loss: 0.7826\n",
      "Epoch 634, Gen Loss: 1.6125, Disc Loss: 0.7160\n",
      "Epoch 635, Gen Loss: 1.6687, Disc Loss: 0.5423\n",
      "Epoch 636, Gen Loss: 1.9781, Disc Loss: 0.5545\n",
      "Epoch 637, Gen Loss: 1.6977, Disc Loss: 0.6151\n",
      "Epoch 638, Gen Loss: 2.0322, Disc Loss: 0.7398\n",
      "Epoch 639, Gen Loss: 1.2363, Disc Loss: 0.7579\n",
      "Epoch 640, Gen Loss: 1.7302, Disc Loss: 0.6584\n",
      "Epoch 641, Gen Loss: 1.4420, Disc Loss: 0.5832\n",
      "Epoch 642, Gen Loss: 2.0482, Disc Loss: 0.4813\n",
      "Epoch 643, Gen Loss: 2.1957, Disc Loss: 0.5565\n",
      "Epoch 644, Gen Loss: 1.9251, Disc Loss: 0.7467\n",
      "Epoch 645, Gen Loss: 1.4354, Disc Loss: 0.9320\n",
      "Epoch 646, Gen Loss: 1.4037, Disc Loss: 1.0672\n",
      "Epoch 647, Gen Loss: 1.6652, Disc Loss: 0.7325\n",
      "Epoch 648, Gen Loss: 1.4652, Disc Loss: 0.6430\n",
      "Epoch 649, Gen Loss: 2.1406, Disc Loss: 0.5586\n",
      "Epoch 650, Gen Loss: 1.9215, Disc Loss: 0.7050\n",
      "Epoch 651, Gen Loss: 1.4485, Disc Loss: 0.7852\n",
      "Epoch 652, Gen Loss: 1.2769, Disc Loss: 0.8262\n",
      "Epoch 653, Gen Loss: 2.7250, Disc Loss: 0.5465\n",
      "Epoch 654, Gen Loss: 2.3860, Disc Loss: 0.2666\n",
      "Epoch 655, Gen Loss: 3.3627, Disc Loss: 0.4979\n",
      "Epoch 656, Gen Loss: 2.2540, Disc Loss: 0.6874\n",
      "Epoch 657, Gen Loss: 1.6401, Disc Loss: 1.3050\n",
      "Epoch 658, Gen Loss: 1.1267, Disc Loss: 1.0601\n",
      "Epoch 659, Gen Loss: 1.6271, Disc Loss: 0.7929\n",
      "Epoch 660, Gen Loss: 1.6558, Disc Loss: 0.6233\n",
      "Epoch 661, Gen Loss: 1.6826, Disc Loss: 0.5149\n",
      "Epoch 662, Gen Loss: 1.9281, Disc Loss: 0.5133\n",
      "Epoch 663, Gen Loss: 1.7598, Disc Loss: 0.6521\n",
      "Epoch 664, Gen Loss: 1.5656, Disc Loss: 1.0348\n",
      "Epoch 665, Gen Loss: 1.2738, Disc Loss: 1.0500\n",
      "Epoch 666, Gen Loss: 1.2842, Disc Loss: 0.7420\n",
      "Epoch 667, Gen Loss: 2.0191, Disc Loss: 0.4736\n",
      "Epoch 668, Gen Loss: 2.4315, Disc Loss: 0.3855\n",
      "Epoch 669, Gen Loss: 1.7263, Disc Loss: 0.5871\n",
      "Epoch 670, Gen Loss: 2.2244, Disc Loss: 0.5198\n",
      "Epoch 671, Gen Loss: 1.7490, Disc Loss: 0.8430\n",
      "Epoch 672, Gen Loss: 1.2406, Disc Loss: 0.8303\n",
      "Epoch 673, Gen Loss: 1.5551, Disc Loss: 0.8817\n",
      "Epoch 674, Gen Loss: 1.4536, Disc Loss: 0.7252\n",
      "Epoch 675, Gen Loss: 2.0766, Disc Loss: 0.5774\n",
      "Epoch 676, Gen Loss: 1.5787, Disc Loss: 0.5261\n",
      "Epoch 677, Gen Loss: 1.8364, Disc Loss: 0.3796\n",
      "Epoch 678, Gen Loss: 1.8927, Disc Loss: 0.5750\n",
      "Epoch 679, Gen Loss: 1.9141, Disc Loss: 0.6884\n",
      "Epoch 680, Gen Loss: 1.3945, Disc Loss: 0.7187\n",
      "Epoch 681, Gen Loss: 1.7418, Disc Loss: 0.5164\n",
      "Epoch 682, Gen Loss: 1.7955, Disc Loss: 0.6894\n",
      "Epoch 683, Gen Loss: 1.9065, Disc Loss: 0.6614\n",
      "Epoch 684, Gen Loss: 2.2459, Disc Loss: 0.7603\n",
      "Epoch 685, Gen Loss: 1.8594, Disc Loss: 0.6096\n",
      "Epoch 686, Gen Loss: 1.6874, Disc Loss: 0.7264\n",
      "Epoch 687, Gen Loss: 1.7859, Disc Loss: 0.7396\n",
      "Epoch 688, Gen Loss: 1.4731, Disc Loss: 0.6498\n",
      "Epoch 689, Gen Loss: 1.9978, Disc Loss: 0.6961\n",
      "Epoch 690, Gen Loss: 1.7701, Disc Loss: 0.6271\n",
      "Epoch 691, Gen Loss: 1.3527, Disc Loss: 0.8173\n",
      "Epoch 692, Gen Loss: 1.1806, Disc Loss: 0.7172\n",
      "Epoch 693, Gen Loss: 1.3762, Disc Loss: 0.6172\n",
      "Epoch 694, Gen Loss: 1.6951, Disc Loss: 0.5792\n",
      "Epoch 695, Gen Loss: 1.7597, Disc Loss: 0.5743\n",
      "Epoch 696, Gen Loss: 1.8400, Disc Loss: 0.7543\n",
      "Epoch 697, Gen Loss: 1.7170, Disc Loss: 0.8885\n",
      "Epoch 698, Gen Loss: 1.1040, Disc Loss: 0.9289\n",
      "Epoch 699, Gen Loss: 1.5581, Disc Loss: 0.6907\n",
      "Epoch 700, Gen Loss: 1.6513, Disc Loss: 0.4808\n",
      "Epoch 701, Gen Loss: 2.0604, Disc Loss: 0.4238\n",
      "Epoch 702, Gen Loss: 1.9147, Disc Loss: 0.7831\n",
      "Epoch 703, Gen Loss: 1.5106, Disc Loss: 0.8740\n",
      "Epoch 704, Gen Loss: 1.2700, Disc Loss: 0.9847\n",
      "Epoch 705, Gen Loss: 1.4169, Disc Loss: 0.6767\n",
      "Epoch 706, Gen Loss: 2.0627, Disc Loss: 0.3790\n",
      "Epoch 707, Gen Loss: 2.0471, Disc Loss: 0.3269\n",
      "Epoch 708, Gen Loss: 2.0665, Disc Loss: 0.4327\n",
      "Epoch 709, Gen Loss: 1.6430, Disc Loss: 0.7839\n",
      "Epoch 710, Gen Loss: 1.3862, Disc Loss: 1.1360\n",
      "Epoch 711, Gen Loss: 1.8247, Disc Loss: 0.5987\n",
      "Epoch 712, Gen Loss: 2.5693, Disc Loss: 0.3770\n",
      "Epoch 713, Gen Loss: 2.3423, Disc Loss: 0.6053\n",
      "Epoch 714, Gen Loss: 1.9081, Disc Loss: 0.8497\n",
      "Epoch 715, Gen Loss: 2.1756, Disc Loss: 1.4630\n",
      "Epoch 716, Gen Loss: 0.7414, Disc Loss: 1.1844\n",
      "Epoch 717, Gen Loss: 1.7602, Disc Loss: 1.0495\n",
      "Epoch 718, Gen Loss: 1.0896, Disc Loss: 0.7336\n",
      "Epoch 719, Gen Loss: 2.6200, Disc Loss: 0.5030\n",
      "Epoch 720, Gen Loss: 1.9318, Disc Loss: 0.4200\n",
      "Epoch 721, Gen Loss: 1.9279, Disc Loss: 0.4949\n",
      "Epoch 722, Gen Loss: 1.8151, Disc Loss: 0.5338\n",
      "Epoch 723, Gen Loss: 1.6354, Disc Loss: 0.8367\n",
      "Epoch 724, Gen Loss: 1.3608, Disc Loss: 1.0632\n",
      "Epoch 725, Gen Loss: 1.0462, Disc Loss: 1.0373\n",
      "Epoch 726, Gen Loss: 1.4407, Disc Loss: 0.8884\n",
      "Epoch 727, Gen Loss: 1.9541, Disc Loss: 0.4316\n",
      "Epoch 728, Gen Loss: 2.4063, Disc Loss: 0.1947\n",
      "Epoch 729, Gen Loss: 2.3938, Disc Loss: 0.2353\n",
      "Epoch 730, Gen Loss: 2.2900, Disc Loss: 0.3423\n",
      "Epoch 731, Gen Loss: 1.8001, Disc Loss: 0.7107\n",
      "Epoch 732, Gen Loss: 1.4380, Disc Loss: 1.1158\n",
      "Epoch 733, Gen Loss: 1.4917, Disc Loss: 0.9146\n",
      "Epoch 734, Gen Loss: 1.3576, Disc Loss: 0.7358\n",
      "Epoch 735, Gen Loss: 1.6946, Disc Loss: 0.5928\n",
      "Epoch 736, Gen Loss: 1.8910, Disc Loss: 0.4139\n",
      "Epoch 737, Gen Loss: 2.0720, Disc Loss: 0.4182\n",
      "Epoch 738, Gen Loss: 2.2449, Disc Loss: 0.3923\n",
      "Epoch 739, Gen Loss: 2.2562, Disc Loss: 0.4263\n",
      "Epoch 740, Gen Loss: 1.5962, Disc Loss: 1.1531\n",
      "Epoch 741, Gen Loss: 1.1390, Disc Loss: 1.0979\n",
      "Epoch 742, Gen Loss: 1.8087, Disc Loss: 0.8492\n",
      "Epoch 743, Gen Loss: 1.7426, Disc Loss: 0.8516\n",
      "Epoch 744, Gen Loss: 1.7813, Disc Loss: 0.6754\n",
      "Epoch 745, Gen Loss: 1.5182, Disc Loss: 0.5249\n",
      "Epoch 746, Gen Loss: 2.5675, Disc Loss: 0.3368\n",
      "Epoch 747, Gen Loss: 1.9335, Disc Loss: 0.4652\n",
      "Epoch 748, Gen Loss: 1.8336, Disc Loss: 0.7697\n",
      "Epoch 749, Gen Loss: 1.6755, Disc Loss: 0.6384\n",
      "Epoch 750, Gen Loss: 1.9703, Disc Loss: 0.4777\n",
      "Epoch 751, Gen Loss: 2.3905, Disc Loss: 0.3998\n",
      "Epoch 752, Gen Loss: 2.4049, Disc Loss: 0.4157\n",
      "Epoch 753, Gen Loss: 2.1786, Disc Loss: 0.6204\n",
      "Epoch 754, Gen Loss: 1.9196, Disc Loss: 0.7946\n",
      "Epoch 755, Gen Loss: 1.4870, Disc Loss: 0.7958\n",
      "Epoch 756, Gen Loss: 1.5555, Disc Loss: 0.7070\n",
      "Epoch 757, Gen Loss: 1.7625, Disc Loss: 0.6316\n",
      "Epoch 758, Gen Loss: 1.4509, Disc Loss: 0.6588\n",
      "Epoch 759, Gen Loss: 2.2347, Disc Loss: 0.5770\n",
      "Epoch 760, Gen Loss: 1.7995, Disc Loss: 0.7815\n",
      "Epoch 761, Gen Loss: 1.5024, Disc Loss: 0.7577\n",
      "Epoch 762, Gen Loss: 1.6380, Disc Loss: 0.7240\n",
      "Epoch 763, Gen Loss: 1.5751, Disc Loss: 0.4752\n",
      "Epoch 764, Gen Loss: 2.2204, Disc Loss: 0.4043\n",
      "Epoch 765, Gen Loss: 1.5011, Disc Loss: 0.4709\n",
      "Epoch 766, Gen Loss: 2.2154, Disc Loss: 0.5239\n",
      "Epoch 767, Gen Loss: 1.6455, Disc Loss: 0.5819\n",
      "Epoch 768, Gen Loss: 2.2530, Disc Loss: 0.8186\n",
      "Epoch 769, Gen Loss: 1.4127, Disc Loss: 0.7797\n",
      "Epoch 770, Gen Loss: 1.4768, Disc Loss: 0.7371\n",
      "Epoch 771, Gen Loss: 1.5914, Disc Loss: 0.6925\n",
      "Epoch 772, Gen Loss: 1.8989, Disc Loss: 0.6328\n",
      "Epoch 773, Gen Loss: 1.4931, Disc Loss: 0.4530\n",
      "Epoch 774, Gen Loss: 2.6938, Disc Loss: 0.4333\n",
      "Epoch 775, Gen Loss: 2.2564, Disc Loss: 0.4528\n",
      "Epoch 776, Gen Loss: 1.5635, Disc Loss: 0.5794\n",
      "Epoch 777, Gen Loss: 2.2653, Disc Loss: 0.9400\n",
      "Epoch 778, Gen Loss: 1.0329, Disc Loss: 0.7713\n",
      "Epoch 779, Gen Loss: 1.9177, Disc Loss: 0.8743\n",
      "Epoch 780, Gen Loss: 1.4662, Disc Loss: 0.7937\n",
      "Epoch 781, Gen Loss: 1.8039, Disc Loss: 0.5198\n",
      "Epoch 782, Gen Loss: 2.1148, Disc Loss: 0.3787\n",
      "Epoch 783, Gen Loss: 2.1369, Disc Loss: 0.5062\n",
      "Epoch 784, Gen Loss: 2.1076, Disc Loss: 0.6501\n",
      "Epoch 785, Gen Loss: 1.6009, Disc Loss: 0.7719\n",
      "Epoch 786, Gen Loss: 1.5460, Disc Loss: 0.8041\n",
      "Epoch 787, Gen Loss: 1.1322, Disc Loss: 0.9764\n",
      "Epoch 788, Gen Loss: 1.6581, Disc Loss: 0.7113\n",
      "Epoch 789, Gen Loss: 1.7820, Disc Loss: 0.4896\n",
      "Epoch 790, Gen Loss: 2.7908, Disc Loss: 0.6666\n",
      "Epoch 791, Gen Loss: 1.5413, Disc Loss: 0.4933\n",
      "Epoch 792, Gen Loss: 2.1496, Disc Loss: 0.6858\n",
      "Epoch 793, Gen Loss: 1.3685, Disc Loss: 0.6456\n",
      "Epoch 794, Gen Loss: 1.8833, Disc Loss: 0.5811\n",
      "Epoch 795, Gen Loss: 1.6413, Disc Loss: 0.7566\n",
      "Epoch 796, Gen Loss: 1.8663, Disc Loss: 0.5865\n",
      "Epoch 797, Gen Loss: 1.7868, Disc Loss: 0.6549\n",
      "Epoch 798, Gen Loss: 1.7945, Disc Loss: 0.5904\n",
      "Epoch 799, Gen Loss: 1.7821, Disc Loss: 0.4950\n",
      "Epoch 800, Gen Loss: 1.5699, Disc Loss: 0.5991\n",
      "Epoch 801, Gen Loss: 1.3255, Disc Loss: 0.7024\n",
      "Epoch 802, Gen Loss: 1.6931, Disc Loss: 0.8625\n",
      "Epoch 803, Gen Loss: 1.6663, Disc Loss: 0.9526\n",
      "Epoch 804, Gen Loss: 2.1672, Disc Loss: 0.8674\n",
      "Epoch 805, Gen Loss: 1.3294, Disc Loss: 0.8482\n",
      "Epoch 806, Gen Loss: 2.0464, Disc Loss: 0.6764\n",
      "Epoch 807, Gen Loss: 1.5403, Disc Loss: 0.6162\n",
      "Epoch 808, Gen Loss: 1.6572, Disc Loss: 0.5780\n",
      "Epoch 809, Gen Loss: 1.4569, Disc Loss: 0.7794\n",
      "Epoch 810, Gen Loss: 1.6991, Disc Loss: 0.7497\n",
      "Epoch 811, Gen Loss: 1.7510, Disc Loss: 0.6977\n",
      "Epoch 812, Gen Loss: 1.7726, Disc Loss: 0.7267\n",
      "Epoch 813, Gen Loss: 1.5736, Disc Loss: 0.5532\n",
      "Epoch 814, Gen Loss: 2.2109, Disc Loss: 0.4928\n",
      "Epoch 815, Gen Loss: 1.5209, Disc Loss: 0.5935\n",
      "Epoch 816, Gen Loss: 2.0348, Disc Loss: 0.6727\n",
      "Epoch 817, Gen Loss: 1.3459, Disc Loss: 0.7604\n",
      "Epoch 818, Gen Loss: 1.6765, Disc Loss: 0.7069\n",
      "Epoch 819, Gen Loss: 1.3757, Disc Loss: 0.6798\n",
      "Epoch 820, Gen Loss: 1.6644, Disc Loss: 0.5925\n",
      "Epoch 821, Gen Loss: 1.5415, Disc Loss: 0.6586\n",
      "Epoch 822, Gen Loss: 1.9821, Disc Loss: 0.4344\n",
      "Epoch 823, Gen Loss: 1.9399, Disc Loss: 0.4888\n",
      "Epoch 824, Gen Loss: 1.9552, Disc Loss: 0.4563\n",
      "Epoch 825, Gen Loss: 2.0929, Disc Loss: 0.6413\n",
      "Epoch 826, Gen Loss: 1.5563, Disc Loss: 0.5692\n",
      "Epoch 827, Gen Loss: 2.2365, Disc Loss: 0.5858\n",
      "Epoch 828, Gen Loss: 1.7033, Disc Loss: 0.4842\n",
      "Epoch 829, Gen Loss: 2.1379, Disc Loss: 0.4889\n",
      "Epoch 830, Gen Loss: 2.4213, Disc Loss: 0.4889\n",
      "Epoch 831, Gen Loss: 1.9272, Disc Loss: 0.5712\n",
      "Epoch 832, Gen Loss: 1.5282, Disc Loss: 0.6993\n",
      "Epoch 833, Gen Loss: 1.5356, Disc Loss: 0.8326\n",
      "Epoch 834, Gen Loss: 1.2406, Disc Loss: 0.8778\n",
      "Epoch 835, Gen Loss: 2.4188, Disc Loss: 0.5931\n",
      "Epoch 836, Gen Loss: 2.2334, Disc Loss: 0.5596\n",
      "Epoch 837, Gen Loss: 1.7874, Disc Loss: 0.5353\n",
      "Epoch 838, Gen Loss: 1.8492, Disc Loss: 0.5762\n",
      "Epoch 839, Gen Loss: 1.8388, Disc Loss: 0.6418\n",
      "Epoch 840, Gen Loss: 1.3695, Disc Loss: 0.6286\n",
      "Epoch 841, Gen Loss: 2.0623, Disc Loss: 0.6068\n",
      "Epoch 842, Gen Loss: 1.8404, Disc Loss: 0.4630\n",
      "Epoch 843, Gen Loss: 1.9924, Disc Loss: 0.4118\n",
      "Epoch 844, Gen Loss: 1.7888, Disc Loss: 0.4237\n",
      "Epoch 845, Gen Loss: 2.0395, Disc Loss: 0.4984\n",
      "Epoch 846, Gen Loss: 1.8482, Disc Loss: 0.6571\n",
      "Epoch 847, Gen Loss: 1.3947, Disc Loss: 0.6368\n",
      "Epoch 848, Gen Loss: 2.0895, Disc Loss: 0.4571\n",
      "Epoch 849, Gen Loss: 1.9290, Disc Loss: 0.4501\n",
      "Epoch 850, Gen Loss: 1.8525, Disc Loss: 0.5326\n",
      "Epoch 851, Gen Loss: 1.6489, Disc Loss: 0.7563\n",
      "Epoch 852, Gen Loss: 1.9025, Disc Loss: 0.9881\n",
      "Epoch 853, Gen Loss: 1.8936, Disc Loss: 1.2442\n",
      "Epoch 854, Gen Loss: 1.1678, Disc Loss: 0.8411\n",
      "Epoch 855, Gen Loss: 1.8661, Disc Loss: 0.5944\n",
      "Epoch 856, Gen Loss: 3.0303, Disc Loss: 0.2682\n",
      "Epoch 857, Gen Loss: 2.0832, Disc Loss: 0.3994\n",
      "Epoch 858, Gen Loss: 2.3846, Disc Loss: 0.4411\n",
      "Epoch 859, Gen Loss: 1.6654, Disc Loss: 0.7057\n",
      "Epoch 860, Gen Loss: 1.2704, Disc Loss: 0.7500\n",
      "Epoch 861, Gen Loss: 1.2969, Disc Loss: 0.7690\n",
      "Epoch 862, Gen Loss: 1.7657, Disc Loss: 0.5201\n",
      "Epoch 863, Gen Loss: 1.8605, Disc Loss: 0.3549\n",
      "Epoch 864, Gen Loss: 2.2292, Disc Loss: 0.4044\n",
      "Epoch 865, Gen Loss: 2.3873, Disc Loss: 0.5513\n",
      "Epoch 866, Gen Loss: 2.6317, Disc Loss: 0.7305\n",
      "Epoch 867, Gen Loss: 1.3745, Disc Loss: 0.7166\n",
      "Epoch 868, Gen Loss: 2.0402, Disc Loss: 0.5948\n",
      "Epoch 869, Gen Loss: 1.6267, Disc Loss: 0.7403\n",
      "Epoch 870, Gen Loss: 1.6197, Disc Loss: 0.7729\n",
      "Epoch 871, Gen Loss: 2.4121, Disc Loss: 0.5642\n",
      "Epoch 872, Gen Loss: 2.3443, Disc Loss: 0.4966\n",
      "Epoch 873, Gen Loss: 2.3883, Disc Loss: 0.4253\n",
      "Epoch 874, Gen Loss: 2.7412, Disc Loss: 0.1969\n",
      "Epoch 875, Gen Loss: 2.6963, Disc Loss: 0.2036\n",
      "Epoch 876, Gen Loss: 2.4098, Disc Loss: 0.4824\n",
      "Epoch 877, Gen Loss: 2.0038, Disc Loss: 0.8130\n",
      "Epoch 878, Gen Loss: 1.0719, Disc Loss: 0.9683\n",
      "Epoch 879, Gen Loss: 2.9083, Disc Loss: 0.7458\n",
      "Epoch 880, Gen Loss: 1.5757, Disc Loss: 0.4156\n",
      "Epoch 881, Gen Loss: 2.2019, Disc Loss: 0.4289\n",
      "Epoch 882, Gen Loss: 3.0834, Disc Loss: 0.2894\n",
      "Epoch 883, Gen Loss: 2.1561, Disc Loss: 0.2564\n",
      "Epoch 884, Gen Loss: 2.5123, Disc Loss: 0.4632\n",
      "Epoch 885, Gen Loss: 2.1181, Disc Loss: 0.5322\n",
      "Epoch 886, Gen Loss: 1.8217, Disc Loss: 0.5586\n",
      "Epoch 887, Gen Loss: 2.0861, Disc Loss: 0.5367\n",
      "Epoch 888, Gen Loss: 1.9938, Disc Loss: 0.4961\n",
      "Epoch 889, Gen Loss: 1.8700, Disc Loss: 0.4626\n",
      "Epoch 890, Gen Loss: 1.8236, Disc Loss: 0.6809\n",
      "Epoch 891, Gen Loss: 1.6381, Disc Loss: 0.6892\n",
      "Epoch 892, Gen Loss: 1.4289, Disc Loss: 0.6540\n",
      "Epoch 893, Gen Loss: 2.0276, Disc Loss: 0.7055\n",
      "Epoch 894, Gen Loss: 1.5639, Disc Loss: 0.7760\n",
      "Epoch 895, Gen Loss: 2.6300, Disc Loss: 0.6908\n",
      "Epoch 896, Gen Loss: 2.1676, Disc Loss: 0.8866\n",
      "Epoch 897, Gen Loss: 3.0829, Disc Loss: 0.5415\n",
      "Epoch 898, Gen Loss: 1.4122, Disc Loss: 0.6703\n",
      "Epoch 899, Gen Loss: 1.8855, Disc Loss: 0.4633\n",
      "Epoch 900, Gen Loss: 2.4686, Disc Loss: 0.4705\n",
      "Epoch 901, Gen Loss: 1.3049, Disc Loss: 0.6747\n",
      "Epoch 902, Gen Loss: 1.7110, Disc Loss: 0.5836\n",
      "Epoch 903, Gen Loss: 1.9009, Disc Loss: 0.4685\n",
      "Epoch 904, Gen Loss: 1.9893, Disc Loss: 0.5059\n",
      "Epoch 905, Gen Loss: 1.7774, Disc Loss: 0.5022\n",
      "Epoch 906, Gen Loss: 1.9576, Disc Loss: 0.5237\n",
      "Epoch 907, Gen Loss: 1.3928, Disc Loss: 0.6777\n",
      "Epoch 908, Gen Loss: 2.0108, Disc Loss: 0.8388\n",
      "Epoch 909, Gen Loss: 1.7053, Disc Loss: 0.9963\n",
      "Epoch 910, Gen Loss: 1.7098, Disc Loss: 0.5531\n",
      "Epoch 911, Gen Loss: 2.5847, Disc Loss: 0.4339\n",
      "Epoch 912, Gen Loss: 2.1952, Disc Loss: 0.4026\n",
      "Epoch 913, Gen Loss: 2.1901, Disc Loss: 0.5020\n",
      "Epoch 914, Gen Loss: 1.6141, Disc Loss: 0.7708\n",
      "Epoch 915, Gen Loss: 1.8627, Disc Loss: 0.5345\n",
      "Epoch 916, Gen Loss: 1.8961, Disc Loss: 0.4655\n",
      "Epoch 917, Gen Loss: 1.8254, Disc Loss: 0.4110\n",
      "Epoch 918, Gen Loss: 2.0820, Disc Loss: 0.6039\n",
      "Epoch 919, Gen Loss: 2.0429, Disc Loss: 0.7320\n",
      "Epoch 920, Gen Loss: 1.6495, Disc Loss: 0.6307\n",
      "Epoch 921, Gen Loss: 1.7197, Disc Loss: 0.6107\n",
      "Epoch 922, Gen Loss: 1.5926, Disc Loss: 0.5388\n",
      "Epoch 923, Gen Loss: 2.1539, Disc Loss: 0.5169\n",
      "Epoch 924, Gen Loss: 1.6482, Disc Loss: 0.5279\n",
      "Epoch 925, Gen Loss: 2.4521, Disc Loss: 0.6712\n",
      "Epoch 926, Gen Loss: 1.8812, Disc Loss: 0.5937\n",
      "Epoch 927, Gen Loss: 1.7202, Disc Loss: 0.6565\n",
      "Epoch 928, Gen Loss: 1.5088, Disc Loss: 0.5663\n",
      "Epoch 929, Gen Loss: 1.9397, Disc Loss: 0.5023\n",
      "Epoch 930, Gen Loss: 1.7066, Disc Loss: 0.4795\n",
      "Epoch 931, Gen Loss: 1.9158, Disc Loss: 0.3823\n",
      "Epoch 932, Gen Loss: 2.2955, Disc Loss: 0.4361\n",
      "Epoch 933, Gen Loss: 1.6427, Disc Loss: 0.6419\n",
      "Epoch 934, Gen Loss: 2.0104, Disc Loss: 0.7511\n",
      "Epoch 935, Gen Loss: 1.6011, Disc Loss: 0.6665\n",
      "Epoch 936, Gen Loss: 1.7020, Disc Loss: 0.5922\n",
      "Epoch 937, Gen Loss: 2.4432, Disc Loss: 0.4042\n",
      "Epoch 938, Gen Loss: 2.0104, Disc Loss: 0.3526\n",
      "Epoch 939, Gen Loss: 2.1501, Disc Loss: 0.4664\n",
      "Epoch 940, Gen Loss: 1.7200, Disc Loss: 0.6162\n",
      "Epoch 941, Gen Loss: 1.6810, Disc Loss: 0.5941\n",
      "Epoch 942, Gen Loss: 1.6697, Disc Loss: 0.4488\n",
      "Epoch 943, Gen Loss: 2.4011, Disc Loss: 0.5101\n",
      "Epoch 944, Gen Loss: 1.8722, Disc Loss: 0.4471\n",
      "Epoch 945, Gen Loss: 1.8845, Disc Loss: 0.6330\n",
      "Epoch 946, Gen Loss: 1.8574, Disc Loss: 0.7048\n",
      "Epoch 947, Gen Loss: 1.7650, Disc Loss: 0.7129\n",
      "Epoch 948, Gen Loss: 1.8547, Disc Loss: 0.5637\n",
      "Epoch 949, Gen Loss: 2.4366, Disc Loss: 0.3834\n",
      "Epoch 950, Gen Loss: 2.3341, Disc Loss: 0.3793\n",
      "Epoch 951, Gen Loss: 2.1052, Disc Loss: 0.4376\n",
      "Epoch 952, Gen Loss: 2.1753, Disc Loss: 0.5267\n",
      "Epoch 953, Gen Loss: 1.6899, Disc Loss: 0.4750\n",
      "Epoch 954, Gen Loss: 1.6435, Disc Loss: 0.5367\n",
      "Epoch 955, Gen Loss: 1.5596, Disc Loss: 0.6193\n",
      "Epoch 956, Gen Loss: 2.0596, Disc Loss: 0.5496\n",
      "Epoch 957, Gen Loss: 1.6264, Disc Loss: 0.5490\n",
      "Epoch 958, Gen Loss: 1.8163, Disc Loss: 0.5241\n",
      "Epoch 959, Gen Loss: 2.1806, Disc Loss: 0.4889\n",
      "Epoch 960, Gen Loss: 1.5892, Disc Loss: 0.5925\n",
      "Epoch 961, Gen Loss: 1.7036, Disc Loss: 0.7575\n",
      "Epoch 962, Gen Loss: 2.2234, Disc Loss: 0.6606\n",
      "Epoch 963, Gen Loss: 2.1779, Disc Loss: 0.4356\n",
      "Epoch 964, Gen Loss: 2.5126, Disc Loss: 0.4494\n",
      "Epoch 965, Gen Loss: 2.0292, Disc Loss: 0.4912\n",
      "Epoch 966, Gen Loss: 2.2839, Disc Loss: 0.6188\n",
      "Epoch 967, Gen Loss: 1.7493, Disc Loss: 0.5133\n",
      "Epoch 968, Gen Loss: 2.3036, Disc Loss: 0.3800\n",
      "Epoch 969, Gen Loss: 1.9305, Disc Loss: 0.3789\n",
      "Epoch 970, Gen Loss: 2.1333, Disc Loss: 0.5610\n",
      "Epoch 971, Gen Loss: 1.9734, Disc Loss: 0.4946\n",
      "Epoch 972, Gen Loss: 1.8402, Disc Loss: 0.7096\n",
      "Epoch 973, Gen Loss: 1.8889, Disc Loss: 0.5602\n",
      "Epoch 974, Gen Loss: 2.2304, Disc Loss: 0.5432\n",
      "Epoch 975, Gen Loss: 1.7654, Disc Loss: 0.5700\n",
      "Epoch 976, Gen Loss: 2.2534, Disc Loss: 0.4194\n",
      "Epoch 977, Gen Loss: 1.9451, Disc Loss: 0.5006\n",
      "Epoch 978, Gen Loss: 1.6105, Disc Loss: 0.4818\n",
      "Epoch 979, Gen Loss: 1.7600, Disc Loss: 0.5696\n",
      "Epoch 980, Gen Loss: 1.9979, Disc Loss: 0.3930\n",
      "Epoch 981, Gen Loss: 2.1600, Disc Loss: 0.4732\n",
      "Epoch 982, Gen Loss: 2.0491, Disc Loss: 0.5292\n",
      "Epoch 983, Gen Loss: 1.8930, Disc Loss: 0.4527\n",
      "Epoch 984, Gen Loss: 1.8700, Disc Loss: 0.5484\n",
      "Epoch 985, Gen Loss: 2.0150, Disc Loss: 0.6987\n",
      "Epoch 986, Gen Loss: 1.3814, Disc Loss: 0.6833\n",
      "Epoch 987, Gen Loss: 1.7760, Disc Loss: 0.5895\n",
      "Epoch 988, Gen Loss: 2.0299, Disc Loss: 0.5380\n",
      "Epoch 989, Gen Loss: 1.9445, Disc Loss: 0.4884\n",
      "Epoch 990, Gen Loss: 1.7388, Disc Loss: 0.4954\n",
      "Epoch 991, Gen Loss: 1.7718, Disc Loss: 0.6695\n",
      "Epoch 992, Gen Loss: 1.6071, Disc Loss: 0.7367\n",
      "Epoch 993, Gen Loss: 1.7241, Disc Loss: 0.5936\n",
      "Epoch 994, Gen Loss: 1.8282, Disc Loss: 0.5617\n",
      "Epoch 995, Gen Loss: 2.2828, Disc Loss: 0.5726\n",
      "Epoch 996, Gen Loss: 1.9041, Disc Loss: 0.5323\n",
      "Epoch 997, Gen Loss: 1.8075, Disc Loss: 0.5385\n",
      "Epoch 998, Gen Loss: 1.8130, Disc Loss: 0.6172\n",
      "Epoch 999, Gen Loss: 1.9766, Disc Loss: 0.6911\n",
      "Epoch 1000, Gen Loss: 1.8335, Disc Loss: 0.5642\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 1001, Gen Loss: 1.9723, Disc Loss: 0.5424\n",
      "Epoch 1002, Gen Loss: 2.5848, Disc Loss: 0.4043\n",
      "Epoch 1003, Gen Loss: 2.1447, Disc Loss: 0.5573\n",
      "Epoch 1004, Gen Loss: 1.8124, Disc Loss: 0.6441\n",
      "Epoch 1005, Gen Loss: 1.7982, Disc Loss: 0.4983\n",
      "Epoch 1006, Gen Loss: 2.0607, Disc Loss: 0.4255\n",
      "Epoch 1007, Gen Loss: 2.3463, Disc Loss: 0.3928\n",
      "Epoch 1008, Gen Loss: 1.9109, Disc Loss: 0.4641\n",
      "Epoch 1009, Gen Loss: 1.6678, Disc Loss: 0.5973\n",
      "Epoch 1010, Gen Loss: 1.7031, Disc Loss: 0.5524\n",
      "Epoch 1011, Gen Loss: 1.7151, Disc Loss: 0.6098\n",
      "Epoch 1012, Gen Loss: 2.2467, Disc Loss: 0.4480\n",
      "Epoch 1013, Gen Loss: 2.0354, Disc Loss: 0.5460\n",
      "Epoch 1014, Gen Loss: 2.2614, Disc Loss: 0.5478\n",
      "Epoch 1015, Gen Loss: 2.0664, Disc Loss: 0.4702\n",
      "Epoch 1016, Gen Loss: 1.8077, Disc Loss: 0.4470\n",
      "Epoch 1017, Gen Loss: 2.0973, Disc Loss: 0.3347\n",
      "Epoch 1018, Gen Loss: 1.8164, Disc Loss: 0.5677\n",
      "Epoch 1019, Gen Loss: 2.4090, Disc Loss: 0.4859\n",
      "Epoch 1020, Gen Loss: 1.9409, Disc Loss: 0.6011\n",
      "Epoch 1021, Gen Loss: 1.6078, Disc Loss: 0.6968\n",
      "Epoch 1022, Gen Loss: 1.6864, Disc Loss: 0.6541\n",
      "Epoch 1023, Gen Loss: 2.1919, Disc Loss: 0.6188\n",
      "Epoch 1024, Gen Loss: 1.2680, Disc Loss: 0.8269\n",
      "Epoch 1025, Gen Loss: 2.5005, Disc Loss: 0.4296\n",
      "Epoch 1026, Gen Loss: 2.1274, Disc Loss: 0.5165\n",
      "Epoch 1027, Gen Loss: 1.8347, Disc Loss: 0.4169\n",
      "Epoch 1028, Gen Loss: 1.9097, Disc Loss: 0.4810\n",
      "Epoch 1029, Gen Loss: 2.2068, Disc Loss: 0.4979\n",
      "Epoch 1030, Gen Loss: 1.9032, Disc Loss: 0.5664\n",
      "Epoch 1031, Gen Loss: 1.6003, Disc Loss: 0.5685\n",
      "Epoch 1032, Gen Loss: 1.8867, Disc Loss: 0.6378\n",
      "Epoch 1033, Gen Loss: 2.0074, Disc Loss: 0.4394\n",
      "Epoch 1034, Gen Loss: 2.0946, Disc Loss: 0.6326\n",
      "Epoch 1035, Gen Loss: 1.6598, Disc Loss: 0.6477\n",
      "Epoch 1036, Gen Loss: 2.3073, Disc Loss: 0.5797\n",
      "Epoch 1037, Gen Loss: 1.3145, Disc Loss: 0.5942\n",
      "Epoch 1038, Gen Loss: 2.6736, Disc Loss: 0.6473\n",
      "Epoch 1039, Gen Loss: 1.7891, Disc Loss: 0.5204\n",
      "Epoch 1040, Gen Loss: 2.0196, Disc Loss: 0.4845\n",
      "Epoch 1041, Gen Loss: 1.9657, Disc Loss: 0.5694\n",
      "Epoch 1042, Gen Loss: 1.7023, Disc Loss: 0.5734\n",
      "Epoch 1043, Gen Loss: 1.9264, Disc Loss: 0.5444\n",
      "Epoch 1044, Gen Loss: 2.1035, Disc Loss: 0.5444\n",
      "Epoch 1045, Gen Loss: 1.9055, Disc Loss: 0.5407\n",
      "Epoch 1046, Gen Loss: 1.8278, Disc Loss: 0.4587\n",
      "Epoch 1047, Gen Loss: 2.0400, Disc Loss: 0.5675\n",
      "Epoch 1048, Gen Loss: 1.8244, Disc Loss: 0.4240\n",
      "Epoch 1049, Gen Loss: 2.0105, Disc Loss: 0.4617\n",
      "Epoch 1050, Gen Loss: 1.9720, Disc Loss: 0.5440\n",
      "Epoch 1051, Gen Loss: 1.7523, Disc Loss: 0.4848\n",
      "Epoch 1052, Gen Loss: 2.2879, Disc Loss: 0.4841\n",
      "Epoch 1053, Gen Loss: 1.6080, Disc Loss: 0.6432\n",
      "Epoch 1054, Gen Loss: 2.5286, Disc Loss: 0.4056\n",
      "Epoch 1055, Gen Loss: 1.7362, Disc Loss: 0.4084\n",
      "Epoch 1056, Gen Loss: 2.1516, Disc Loss: 0.4800\n",
      "Epoch 1057, Gen Loss: 1.6993, Disc Loss: 0.4849\n",
      "Epoch 1058, Gen Loss: 2.0496, Disc Loss: 0.5157\n",
      "Epoch 1059, Gen Loss: 2.0408, Disc Loss: 0.4550\n",
      "Epoch 1060, Gen Loss: 1.6964, Disc Loss: 0.5188\n",
      "Epoch 1061, Gen Loss: 2.2234, Disc Loss: 0.4428\n",
      "Epoch 1062, Gen Loss: 2.3827, Disc Loss: 0.5025\n",
      "Epoch 1063, Gen Loss: 1.7151, Disc Loss: 0.4448\n",
      "Epoch 1064, Gen Loss: 2.4880, Disc Loss: 0.4623\n",
      "Epoch 1065, Gen Loss: 1.4326, Disc Loss: 0.5361\n",
      "Epoch 1066, Gen Loss: 2.0223, Disc Loss: 0.5920\n",
      "Epoch 1067, Gen Loss: 2.1022, Disc Loss: 0.4541\n",
      "Epoch 1068, Gen Loss: 1.9381, Disc Loss: 0.4209\n",
      "Epoch 1069, Gen Loss: 2.2618, Disc Loss: 0.4906\n",
      "Epoch 1070, Gen Loss: 2.1792, Disc Loss: 0.5196\n",
      "Epoch 1071, Gen Loss: 1.8802, Disc Loss: 0.5755\n",
      "Epoch 1072, Gen Loss: 1.5998, Disc Loss: 0.5510\n",
      "Epoch 1073, Gen Loss: 2.2928, Disc Loss: 0.5002\n",
      "Epoch 1074, Gen Loss: 1.9884, Disc Loss: 0.4573\n",
      "Epoch 1075, Gen Loss: 2.8080, Disc Loss: 0.4527\n",
      "Epoch 1076, Gen Loss: 1.5525, Disc Loss: 0.5257\n",
      "Epoch 1077, Gen Loss: 1.7399, Disc Loss: 0.4685\n",
      "Epoch 1078, Gen Loss: 2.6342, Disc Loss: 0.4012\n",
      "Epoch 1079, Gen Loss: 1.9478, Disc Loss: 0.3446\n",
      "Epoch 1080, Gen Loss: 2.6365, Disc Loss: 0.4774\n",
      "Epoch 1081, Gen Loss: 1.7335, Disc Loss: 0.3587\n",
      "Epoch 1082, Gen Loss: 2.3104, Disc Loss: 0.4978\n",
      "Epoch 1083, Gen Loss: 2.0854, Disc Loss: 0.3770\n",
      "Epoch 1084, Gen Loss: 2.3000, Disc Loss: 0.4775\n",
      "Epoch 1085, Gen Loss: 1.9113, Disc Loss: 0.4858\n",
      "Epoch 1086, Gen Loss: 2.1002, Disc Loss: 0.4490\n",
      "Epoch 1087, Gen Loss: 2.0495, Disc Loss: 0.4175\n",
      "Epoch 1088, Gen Loss: 2.6006, Disc Loss: 0.4542\n",
      "Epoch 1089, Gen Loss: 2.0301, Disc Loss: 0.3895\n",
      "Epoch 1090, Gen Loss: 1.8126, Disc Loss: 0.4896\n",
      "Epoch 1091, Gen Loss: 1.7945, Disc Loss: 0.5823\n",
      "Epoch 1092, Gen Loss: 2.3549, Disc Loss: 0.4736\n",
      "Epoch 1093, Gen Loss: 1.9961, Disc Loss: 0.4855\n",
      "Epoch 1094, Gen Loss: 3.2670, Disc Loss: 0.3332\n",
      "Epoch 1095, Gen Loss: 2.0033, Disc Loss: 0.4897\n",
      "Epoch 1096, Gen Loss: 1.9754, Disc Loss: 0.5674\n",
      "Epoch 1097, Gen Loss: 1.8531, Disc Loss: 0.6671\n",
      "Epoch 1098, Gen Loss: 1.7659, Disc Loss: 0.6203\n",
      "Epoch 1099, Gen Loss: 1.9828, Disc Loss: 0.4982\n",
      "Epoch 1100, Gen Loss: 2.0938, Disc Loss: 0.3295\n",
      "Epoch 1101, Gen Loss: 2.1852, Disc Loss: 0.4470\n",
      "Epoch 1102, Gen Loss: 1.5556, Disc Loss: 0.4361\n",
      "Epoch 1103, Gen Loss: 2.4427, Disc Loss: 0.4632\n",
      "Epoch 1104, Gen Loss: 2.3573, Disc Loss: 0.4877\n",
      "Epoch 1105, Gen Loss: 2.1866, Disc Loss: 0.3833\n",
      "Epoch 1106, Gen Loss: 2.3854, Disc Loss: 0.3734\n",
      "Epoch 1107, Gen Loss: 2.3708, Disc Loss: 0.2853\n",
      "Epoch 1108, Gen Loss: 2.1076, Disc Loss: 0.4047\n",
      "Epoch 1109, Gen Loss: 2.5122, Disc Loss: 0.4505\n",
      "Epoch 1110, Gen Loss: 1.5897, Disc Loss: 0.6835\n",
      "Epoch 1111, Gen Loss: 2.5518, Disc Loss: 0.5938\n",
      "Epoch 1112, Gen Loss: 1.8418, Disc Loss: 0.3070\n",
      "Epoch 1113, Gen Loss: 2.9174, Disc Loss: 0.3617\n",
      "Epoch 1114, Gen Loss: 2.1157, Disc Loss: 0.4234\n",
      "Epoch 1115, Gen Loss: 1.7768, Disc Loss: 0.4943\n",
      "Epoch 1116, Gen Loss: 1.9347, Disc Loss: 0.6210\n",
      "Epoch 1117, Gen Loss: 1.8145, Disc Loss: 0.4988\n",
      "Epoch 1118, Gen Loss: 2.4006, Disc Loss: 0.3267\n",
      "Epoch 1119, Gen Loss: 2.6899, Disc Loss: 0.5107\n",
      "Epoch 1120, Gen Loss: 2.9561, Disc Loss: 0.2910\n",
      "Epoch 1121, Gen Loss: 2.2692, Disc Loss: 0.4730\n",
      "Epoch 1122, Gen Loss: 2.2734, Disc Loss: 0.4496\n",
      "Epoch 1123, Gen Loss: 2.0416, Disc Loss: 0.5598\n",
      "Epoch 1124, Gen Loss: 3.3759, Disc Loss: 0.5048\n",
      "Epoch 1125, Gen Loss: 1.6463, Disc Loss: 0.4670\n",
      "Epoch 1126, Gen Loss: 2.2509, Disc Loss: 0.4306\n",
      "Epoch 1127, Gen Loss: 2.3053, Disc Loss: 0.2880\n",
      "Epoch 1128, Gen Loss: 2.2562, Disc Loss: 0.5149\n",
      "Epoch 1129, Gen Loss: 1.8796, Disc Loss: 0.4682\n",
      "Epoch 1130, Gen Loss: 1.5070, Disc Loss: 0.6631\n",
      "Epoch 1131, Gen Loss: 1.7769, Disc Loss: 0.4672\n",
      "Epoch 1132, Gen Loss: 2.3284, Disc Loss: 0.3824\n",
      "Epoch 1133, Gen Loss: 2.4706, Disc Loss: 0.3478\n",
      "Epoch 1134, Gen Loss: 2.3072, Disc Loss: 0.4031\n",
      "Epoch 1135, Gen Loss: 2.4973, Disc Loss: 0.5849\n",
      "Epoch 1136, Gen Loss: 1.6883, Disc Loss: 0.5058\n",
      "Epoch 1137, Gen Loss: 2.5271, Disc Loss: 0.3318\n",
      "Epoch 1138, Gen Loss: 2.4153, Disc Loss: 0.3232\n",
      "Epoch 1139, Gen Loss: 2.2832, Disc Loss: 0.3153\n",
      "Epoch 1140, Gen Loss: 2.5158, Disc Loss: 0.3002\n",
      "Epoch 1141, Gen Loss: 2.1552, Disc Loss: 0.4185\n",
      "Epoch 1142, Gen Loss: 2.2237, Disc Loss: 0.4497\n",
      "Epoch 1143, Gen Loss: 1.8985, Disc Loss: 0.4730\n",
      "Epoch 1144, Gen Loss: 2.1186, Disc Loss: 0.5238\n",
      "Epoch 1145, Gen Loss: 1.7291, Disc Loss: 0.5293\n",
      "Epoch 1146, Gen Loss: 2.6139, Disc Loss: 0.2952\n",
      "Epoch 1147, Gen Loss: 2.1548, Disc Loss: 0.2967\n",
      "Epoch 1148, Gen Loss: 3.1467, Disc Loss: 0.3704\n",
      "Epoch 1149, Gen Loss: 2.6005, Disc Loss: 0.3732\n",
      "Epoch 1150, Gen Loss: 1.7912, Disc Loss: 0.3957\n",
      "Epoch 1151, Gen Loss: 2.5111, Disc Loss: 0.4699\n",
      "Epoch 1152, Gen Loss: 1.9871, Disc Loss: 0.4997\n",
      "Epoch 1153, Gen Loss: 2.1225, Disc Loss: 0.3803\n",
      "Epoch 1154, Gen Loss: 2.1881, Disc Loss: 0.4546\n",
      "Epoch 1155, Gen Loss: 2.3167, Disc Loss: 0.3894\n",
      "Epoch 1156, Gen Loss: 2.6406, Disc Loss: 0.3795\n",
      "Epoch 1157, Gen Loss: 2.1316, Disc Loss: 0.3389\n",
      "Epoch 1158, Gen Loss: 2.2397, Disc Loss: 0.5538\n",
      "Epoch 1159, Gen Loss: 2.0158, Disc Loss: 0.3961\n",
      "Epoch 1160, Gen Loss: 2.4432, Disc Loss: 0.4215\n",
      "Epoch 1161, Gen Loss: 2.4128, Disc Loss: 0.2310\n",
      "Epoch 1162, Gen Loss: 2.8724, Disc Loss: 0.2994\n",
      "Epoch 1163, Gen Loss: 2.1771, Disc Loss: 0.3449\n",
      "Epoch 1164, Gen Loss: 2.4703, Disc Loss: 0.5732\n",
      "Epoch 1165, Gen Loss: 2.2696, Disc Loss: 0.4563\n",
      "Epoch 1166, Gen Loss: 2.9036, Disc Loss: 0.2995\n",
      "Epoch 1167, Gen Loss: 2.4271, Disc Loss: 0.2847\n",
      "Epoch 1168, Gen Loss: 2.7519, Disc Loss: 0.2642\n",
      "Epoch 1169, Gen Loss: 2.5172, Disc Loss: 0.4046\n",
      "Epoch 1170, Gen Loss: 2.5243, Disc Loss: 0.4885\n",
      "Epoch 1171, Gen Loss: 2.3926, Disc Loss: 0.6033\n",
      "Epoch 1172, Gen Loss: 1.5336, Disc Loss: 0.6435\n",
      "Epoch 1173, Gen Loss: 3.3258, Disc Loss: 0.5400\n",
      "Epoch 1174, Gen Loss: 1.2761, Disc Loss: 0.5737\n",
      "Epoch 1175, Gen Loss: 2.7485, Disc Loss: 0.3330\n",
      "Epoch 1176, Gen Loss: 2.8958, Disc Loss: 0.5925\n",
      "Epoch 1177, Gen Loss: 2.2358, Disc Loss: 0.6789\n",
      "Epoch 1178, Gen Loss: 2.9492, Disc Loss: 0.4736\n",
      "Epoch 1179, Gen Loss: 2.0072, Disc Loss: 0.5173\n",
      "Epoch 1180, Gen Loss: 1.8815, Disc Loss: 0.4631\n",
      "Epoch 1181, Gen Loss: 2.4768, Disc Loss: 0.4374\n",
      "Epoch 1182, Gen Loss: 1.5701, Disc Loss: 0.5120\n",
      "Epoch 1183, Gen Loss: 2.5072, Disc Loss: 0.4426\n",
      "Epoch 1184, Gen Loss: 3.1480, Disc Loss: 0.4411\n",
      "Epoch 1185, Gen Loss: 1.6629, Disc Loss: 0.3889\n",
      "Epoch 1186, Gen Loss: 2.6363, Disc Loss: 0.2730\n",
      "Epoch 1187, Gen Loss: 2.7888, Disc Loss: 0.3148\n",
      "Epoch 1188, Gen Loss: 1.9722, Disc Loss: 0.3922\n",
      "Epoch 1189, Gen Loss: 2.5126, Disc Loss: 0.4033\n",
      "Epoch 1190, Gen Loss: 1.8931, Disc Loss: 0.3788\n",
      "Epoch 1191, Gen Loss: 2.2089, Disc Loss: 0.3412\n",
      "Epoch 1192, Gen Loss: 2.2183, Disc Loss: 0.3911\n",
      "Epoch 1193, Gen Loss: 2.5643, Disc Loss: 0.2977\n",
      "Epoch 1194, Gen Loss: 2.9842, Disc Loss: 0.4144\n",
      "Epoch 1195, Gen Loss: 2.1080, Disc Loss: 0.4131\n",
      "Epoch 1196, Gen Loss: 2.1757, Disc Loss: 0.4435\n",
      "Epoch 1197, Gen Loss: 1.8176, Disc Loss: 0.4230\n",
      "Epoch 1198, Gen Loss: 3.1510, Disc Loss: 0.4086\n",
      "Epoch 1199, Gen Loss: 2.1788, Disc Loss: 0.3567\n",
      "Epoch 1200, Gen Loss: 3.3594, Disc Loss: 0.2980\n",
      "Epoch 1201, Gen Loss: 2.1180, Disc Loss: 0.4025\n",
      "Epoch 1202, Gen Loss: 2.8201, Disc Loss: 0.3699\n",
      "Epoch 1203, Gen Loss: 1.6425, Disc Loss: 0.4826\n",
      "Epoch 1204, Gen Loss: 2.5533, Disc Loss: 0.6059\n",
      "Epoch 1205, Gen Loss: 1.9255, Disc Loss: 0.4154\n",
      "Epoch 1206, Gen Loss: 2.8924, Disc Loss: 0.4172\n",
      "Epoch 1207, Gen Loss: 2.2370, Disc Loss: 0.3941\n",
      "Epoch 1208, Gen Loss: 2.8743, Disc Loss: 0.3637\n",
      "Epoch 1209, Gen Loss: 2.3443, Disc Loss: 0.3653\n",
      "Epoch 1210, Gen Loss: 2.2334, Disc Loss: 0.3523\n",
      "Epoch 1211, Gen Loss: 2.4043, Disc Loss: 0.3527\n",
      "Epoch 1212, Gen Loss: 2.3092, Disc Loss: 0.3560\n",
      "Epoch 1213, Gen Loss: 2.2324, Disc Loss: 0.4206\n",
      "Epoch 1214, Gen Loss: 2.4299, Disc Loss: 0.2394\n",
      "Epoch 1215, Gen Loss: 2.7795, Disc Loss: 0.2309\n",
      "Epoch 1216, Gen Loss: 2.2452, Disc Loss: 0.3168\n",
      "Epoch 1217, Gen Loss: 2.5205, Disc Loss: 0.4117\n",
      "Epoch 1218, Gen Loss: 2.0005, Disc Loss: 0.5339\n",
      "Epoch 1219, Gen Loss: 2.2156, Disc Loss: 0.4589\n",
      "Epoch 1220, Gen Loss: 2.3806, Disc Loss: 0.2948\n",
      "Epoch 1221, Gen Loss: 2.8574, Disc Loss: 0.3007\n",
      "Epoch 1222, Gen Loss: 2.5385, Disc Loss: 0.5118\n",
      "Epoch 1223, Gen Loss: 2.2840, Disc Loss: 0.3538\n",
      "Epoch 1224, Gen Loss: 2.6378, Disc Loss: 0.3242\n",
      "Epoch 1225, Gen Loss: 1.9144, Disc Loss: 0.4178\n",
      "Epoch 1226, Gen Loss: 2.7187, Disc Loss: 0.3213\n",
      "Epoch 1227, Gen Loss: 2.0397, Disc Loss: 0.4255\n",
      "Epoch 1228, Gen Loss: 2.7891, Disc Loss: 0.3369\n",
      "Epoch 1229, Gen Loss: 1.9664, Disc Loss: 0.4165\n",
      "Epoch 1230, Gen Loss: 3.4102, Disc Loss: 0.3500\n",
      "Epoch 1231, Gen Loss: 2.2628, Disc Loss: 0.3424\n",
      "Epoch 1232, Gen Loss: 2.4492, Disc Loss: 0.5030\n",
      "Epoch 1233, Gen Loss: 1.9792, Disc Loss: 0.4255\n",
      "Epoch 1234, Gen Loss: 2.2223, Disc Loss: 0.3472\n",
      "Epoch 1235, Gen Loss: 2.5148, Disc Loss: 0.2823\n",
      "Epoch 1236, Gen Loss: 2.6473, Disc Loss: 0.2085\n",
      "Epoch 1237, Gen Loss: 2.9453, Disc Loss: 0.2341\n",
      "Epoch 1238, Gen Loss: 2.5242, Disc Loss: 0.2837\n",
      "Epoch 1239, Gen Loss: 2.6983, Disc Loss: 0.4902\n",
      "Epoch 1240, Gen Loss: 1.8742, Disc Loss: 0.4104\n",
      "Epoch 1241, Gen Loss: 2.9095, Disc Loss: 0.2951\n",
      "Epoch 1242, Gen Loss: 2.1920, Disc Loss: 0.4195\n",
      "Epoch 1243, Gen Loss: 2.5141, Disc Loss: 0.3402\n",
      "Epoch 1244, Gen Loss: 2.2455, Disc Loss: 0.3019\n",
      "Epoch 1245, Gen Loss: 2.4094, Disc Loss: 0.3754\n",
      "Epoch 1246, Gen Loss: 2.7267, Disc Loss: 0.2386\n",
      "Epoch 1247, Gen Loss: 2.6856, Disc Loss: 0.3707\n",
      "Epoch 1248, Gen Loss: 2.6852, Disc Loss: 0.3911\n",
      "Epoch 1249, Gen Loss: 2.7224, Disc Loss: 0.5144\n",
      "Epoch 1250, Gen Loss: 1.9313, Disc Loss: 0.3913\n",
      "Epoch 1251, Gen Loss: 2.7414, Disc Loss: 0.4047\n",
      "Epoch 1252, Gen Loss: 2.3658, Disc Loss: 0.4361\n",
      "Epoch 1253, Gen Loss: 2.5775, Disc Loss: 0.3376\n",
      "Epoch 1254, Gen Loss: 3.0822, Disc Loss: 0.3029\n",
      "Epoch 1255, Gen Loss: 2.2153, Disc Loss: 0.4254\n",
      "Epoch 1256, Gen Loss: 2.7602, Disc Loss: 0.3306\n",
      "Epoch 1257, Gen Loss: 1.8645, Disc Loss: 0.3451\n",
      "Epoch 1258, Gen Loss: 3.6765, Disc Loss: 0.3840\n",
      "Epoch 1259, Gen Loss: 1.7933, Disc Loss: 0.3745\n",
      "Epoch 1260, Gen Loss: 3.0735, Disc Loss: 0.5712\n",
      "Epoch 1261, Gen Loss: 1.7538, Disc Loss: 0.4403\n",
      "Epoch 1262, Gen Loss: 1.8636, Disc Loss: 0.5522\n",
      "Epoch 1263, Gen Loss: 3.0216, Disc Loss: 0.3925\n",
      "Epoch 1264, Gen Loss: 2.2346, Disc Loss: 0.4502\n",
      "Epoch 1265, Gen Loss: 2.4257, Disc Loss: 0.2772\n",
      "Epoch 1266, Gen Loss: 2.8692, Disc Loss: 0.2483\n",
      "Epoch 1267, Gen Loss: 2.5873, Disc Loss: 0.2798\n",
      "Epoch 1268, Gen Loss: 2.3341, Disc Loss: 0.3579\n",
      "Epoch 1269, Gen Loss: 2.3345, Disc Loss: 0.3792\n",
      "Epoch 1270, Gen Loss: 3.0822, Disc Loss: 0.4263\n",
      "Epoch 1271, Gen Loss: 2.1799, Disc Loss: 0.3041\n",
      "Epoch 1272, Gen Loss: 3.1203, Disc Loss: 0.2507\n",
      "Epoch 1273, Gen Loss: 2.2675, Disc Loss: 0.3489\n",
      "Epoch 1274, Gen Loss: 2.5839, Disc Loss: 0.2615\n",
      "Epoch 1275, Gen Loss: 2.4698, Disc Loss: 0.2818\n",
      "Epoch 1276, Gen Loss: 2.1109, Disc Loss: 0.4541\n",
      "Epoch 1277, Gen Loss: 2.4792, Disc Loss: 0.3005\n",
      "Epoch 1278, Gen Loss: 2.6878, Disc Loss: 0.3106\n",
      "Epoch 1279, Gen Loss: 2.2395, Disc Loss: 0.3701\n",
      "Epoch 1280, Gen Loss: 3.1052, Disc Loss: 0.2958\n",
      "Epoch 1281, Gen Loss: 1.7895, Disc Loss: 0.4454\n",
      "Epoch 1282, Gen Loss: 3.2262, Disc Loss: 0.2298\n",
      "Epoch 1283, Gen Loss: 2.4773, Disc Loss: 0.2622\n",
      "Epoch 1284, Gen Loss: 2.7324, Disc Loss: 0.2737\n",
      "Epoch 1285, Gen Loss: 2.6549, Disc Loss: 0.2670\n",
      "Epoch 1286, Gen Loss: 2.5575, Disc Loss: 0.2626\n",
      "Epoch 1287, Gen Loss: 2.5627, Disc Loss: 0.2946\n",
      "Epoch 1288, Gen Loss: 2.7620, Disc Loss: 0.3043\n",
      "Epoch 1289, Gen Loss: 2.6368, Disc Loss: 0.3319\n",
      "Epoch 1290, Gen Loss: 2.6468, Disc Loss: 0.2870\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "# generator = tf.keras.models.load_model('generator_model_epoch_1000.h5')\n",
    "# discriminator = tf.keras.models.load_model('discriminator_model_epoch_1000.h5')\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "\n",
    "# Load and prepare the dataset\n",
    "data_dir = \"dataset_cbp\"\n",
    "train_dataset = load_and_preprocess_dataset(data_dir, image_size=img_sz)\n",
    "\n",
    "# Create a seed for image generation\n",
    "seed = tf.random.normal([16, noise_dim])\n",
    "\n",
    "# Train the model\n",
    "train(train_dataset, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
